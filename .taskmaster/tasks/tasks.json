{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Setup Core Utilities Module",
        "description": "Establish the foundational `utils` module for shared data types, configuration management, and logging, essential for all other modules.",
        "details": "This task involves setting up the `utils` directory and implementing its core components:\n\n1.  **data_types.py**: Define common data structures and type hints using Python's `dataclasses` (e.g., `FuturesData`, `HMMState`, `BacktestResult`, `PerformanceMetrics`). Leverage `numpy` and `pandas` for array and series types. Example: `@dataclass class FuturesData: timestamp: datetime; open: float; high: float; low: float; close: float; volume: float`.\n2.  **config.py**: Implement a robust configuration management system using Python's `dataclasses` for structured configuration objects (`HMMConfig`, `ProcessingConfig`). Utilize `pydantic` (version 2.5.3+) for schema validation of configuration files (e.g., YAML/JSON) to ensure type safety and sensible defaults. This enhances `validate_config` functionality. Example:\n    ```python\n    from pydantic import BaseModel, Field\n    from typing import Dict, Any\n\n    class HMMConfig(BaseModel):\n        n_states: int = Field(3, ge=1, le=10) # Example validation\n        covariance_type: str = 'diag'\n        max_iter: int = 100\n        random_state: int = 42\n        tol: float = 1e-3\n\n    def load_config(config_path: str) -> HMMConfig:\n        # Load YAML/JSON and validate with HMMConfig.parse_obj()\n        pass\n    ```\n3.  **logging_config.py**: Configure a structured logging system using Python's `logging` module. Implement `setup_logging` to handle different log levels (DEBUG, INFO, WARNING, ERROR) and output formats (console, file). Consider using `loguru` (version 0.7.2+) for more developer-friendly logging with automatic rotation and colorization, or `structlog` for truly structured JSON logging if deeper integration with monitoring systems is planned.\n\n**Recommended Libraries & Versions:**\n*   `python`: 3.10+\n*   `dataclasses`: Built-in\n*   `pydantic`: 2.5.3+\n*   `loguru`: 0.7.2+ (or built-in `logging` module)\n*   `numpy`: 1.26.2+\n*   `pandas`: 2.1.3+\n\n**Environment Setup:** Initialize the project using `Poetry` (version 1.7.0+) for dependency management and virtual environment setup. Create `pyproject.toml` and add initial dependencies.",
        "testStrategy": "Unit tests for each utility function (e.g., `test_load_config_valid`, `test_load_config_invalid`, `test_setup_logging_output`). Ensure type hints are correctly applied and validated by `mypy`. Achieve >90% line coverage for the `utils` module. Critical scenarios: Configuration loading with missing or invalid fields, logging output at different levels.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Project and Setup Core Dependencies",
            "description": "Set up the project structure using Poetry, create the `pyproject.toml` file, and add essential libraries like `pydantic`, `numpy`, and `pandas` as specified, including creating the `utils` directory.",
            "dependencies": [],
            "details": "Use `Poetry new project_name` to initialize the project (if not already done). Configure `pyproject.toml` to specify Python 3.10+. Add `pydantic` (version 2.5.3+), `numpy` (version 1.26.2+), and `pandas` (version 2.1.3+) to `[tool.poetry.dependencies]`. Run `poetry install`. Create the `src/utils` directory structure within the project.",
            "status": "done",
            "testStrategy": "Verify `poetry env info` shows the correct Python version. Run `poetry install` and ensure all dependencies are resolved without errors. Check for the existence of `pyproject.toml` and the `src/utils` directory. Confirm that `pydantic`, `numpy`, and `pandas` are listed in `poetry show` output."
          },
          {
            "id": 2,
            "title": "Develop Data Types and Configuration Management Modules",
            "description": "Create `data_types.py` to define core data structures using Python's `dataclasses` and `config.py` to implement a Pydantic-validated configuration system for loading and managing application settings.",
            "dependencies": [
              1
            ],
            "details": "In `src/utils/data_types.py`, define `dataclasses` like `FuturesData` (timestamp: datetime, open: float, high: float, low: float, close: float, volume: float), `HMMState`, `BacktestResult`, and `PerformanceMetrics`. Leverage `numpy` and `pandas` for appropriate array/series types where applicable. In `src/utils/config.py`, implement `HMMConfig` and `ProcessingConfig` using `pydantic.BaseModel` with `Field` validation (e.g., `n_states: int = Field(3, ge=1, le=10)`). Add a `load_config(config_path: str) -> HMMConfig` (or similar for ProcessingConfig) function to load YAML/JSON files and validate them using `Pydantic`'s parsing capabilities.",
            "status": "done",
            "testStrategy": "Unit tests for `config.py`: `test_load_config_valid_yaml`, `test_load_config_invalid_schema`, `test_default_config_values`. Ensure `mypy` passes for type checking across `data_types.py` and `config.py`. Manually inspect `dataclasses` and `pydantic` models for correct field definitions, types, and default values. Create example config files (valid/invalid) to test loading logic."
          },
          {
            "id": 3,
            "title": "Implement Structured Logging Configuration",
            "description": "Establish a robust and structured logging system for the `utils` module and the overall project, handling different log levels, output formats, and optional advanced features.",
            "dependencies": [
              1
            ],
            "details": "Create `src/utils/logging_config.py`. Implement a `setup_logging()` function that configures Python's built-in `logging` module. This function should handle different log levels (DEBUG, INFO, WARNING, ERROR) and output formats for both console and a file. Consider integrating `loguru` (version 0.7.2+) for more developer-friendly logging features like automatic rotation and colorization, or `structlog` for structured JSON output if deeper integration with monitoring systems is planned.",
            "status": "done",
            "testStrategy": "Unit tests for `logging_config.py`: `test_setup_logging_console_output`, `test_setup_logging_file_output`, `test_log_levels_filtering`. Verify that log messages appear in the correct format and destination (console, log file) for different severity levels. Check for log file creation and rotation if those features are implemented."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Data Processing & Feature Engineering Module",
        "description": "Develop the `data_processing` module to transform raw futures OHLCV data into clean, feature-rich datasets, handling multi-format CSVs, technical indicator calculation, and data validation/cleaning.",
        "details": "This task focuses on implementing the core data processing capabilities:\n\n1.  **csv_parser.py**: Create `process_csv` function to parse multi-format CSVs. Use `pandas.read_csv` with `chunksize` for memory efficiency. Implement logic to detect date/time column formats (e.g., 'YYYY-MM-DD HH:MM:SS' vs 'YYYY-MM-DD' and 'HH:MM:SS' separately) using `pd.to_datetime` with `infer_datetime_format=True`. Handle column renaming to a standardized OHLCV format, strip whitespace, and downcast numerical dtypes to `float32` (or `float16` if precision allows) using `df.astype()` to minimize memory footprint. Implement optional symbol filtering.\n2.  **feature_engineering.py**: Implement `add_features` to compute 11+ technical indicators using `pandas_ta` (version 0.3.14b0+). This includes log returns, ATR, ROC, RSI, Bollinger Bands, ADX, Stochastic, SMA ratios, and volume metrics. Ensure rolling window calculations handle `min_periods` correctly and avoid NaNs propagating unnecessarily. Leverage vectorized `pandas` operations for performance. The `indicator_config` from `utils.config` will specify which indicators to compute and their parameters.\n3.  **data_validation.py**: Implement `validate_data` function for data quality checks. This includes verifying required OHLCV columns, validating data types (e.g., numerical for prices, datetime for index), identifying and handling missing values (e.g., forward-fill or drop, depending on context), and detecting/logging outliers (e.g., using IQR or Z-score methods). Output a data quality report.\n\n**Pseudo-code for `add_features`:**\n```python\nimport pandas_ta as ta\nimport pandas as pd\n\ndef add_features(df: pd.DataFrame, indicator_config: dict) -> pd.DataFrame:\n    # Ensure datetime index\n    df.index = pd.to_datetime(df.index)\n    # Calculate log returns\n    df['log_ret'] = ta.log_return(df['close'], append=False)\n    # Calculate various indicators based on config\n    for indicator, params in indicator_config.items():\n        if hasattr(ta, indicator.lower()):\n            indicator_func = getattr(ta, indicator.lower())\n            df = pd.concat([df, indicator_func(df['high'], df['low'], df['close'], **params)], axis=1)\n    \n    # Downcast features to float32 after calculation\n    for col in df.columns:\n        if df[col].dtype == 'float64':\n            df[col] = df[col].astype('float32')\n    return df\n```\n\n**Recommended Libraries & Versions:**\n*   `pandas`: 2.1.3+\n*   `numpy`: 1.26.2+\n*   `pandas_ta`: 0.3.14b0+\n*   `scikit-learn`: 1.3.2+ (for preprocessing utilities like `StandardScaler` later, but also for `preprocessing.RobustScaler` or `isolationForest` for outlier detection if needed here).",
        "testStrategy": "Unit tests for `csv_parser.py` with various CSV formats (different delimiters, column names, missing data) and symbol filters. Property-based tests (using `hypothesis` 6.94.0+) for `feature_engineering.py` to validate indicator calculations across wide ranges of synthetic price data. Integration tests for `data_validation.py` to ensure robust handling of corrupted data and correct error reporting. Critical scenarios: Parsing multi-GB files with low memory, accurate indicator calculation at boundaries (e.g., `min_periods`), graceful handling of missing required columns.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement `csv_parser.py` for multi-format CSV loading",
            "description": "Develop the `process_csv` function to efficiently parse multi-format OHLCV CSV files, detect and standardize datetime columns, rename columns, strip whitespace, downcast numerical data types for memory efficiency, and optionally filter by symbol.",
            "dependencies": [],
            "details": "Create `csv_parser.py` and implement `process_csv`. Utilize `pandas.read_csv` with `chunksize` for memory-efficient loading. Implement logic to detect and convert various date/time column formats (e.g., 'YYYY-MM-DD HH:MM:SS' vs 'YYYY-MM-DD' and 'HH:MM:SS' separately) using `pd.to_datetime(..., infer_datetime_format=True)`. Rename columns to a standardized OHLCV format (e.g., 'date', 'open', 'high', 'low', 'close', 'volume'). Strip leading/trailing whitespace from column names and string data. Downcast numerical columns to `float32` (or `float16` if suitable) and integer columns to `int32`/`int16` using `df.astype()` to minimize memory footprint. Add a parameter for optional symbol filtering.",
            "status": "done",
            "testStrategy": "Unit tests for `process_csv` with various CSV formats (e.g., different delimiters like comma/semicolon, different date/time column arrangements, missing date/time components, varying column names like 'open_price', 'Close', 'Date Time'), including tests for symbol filtering, data type downcasting, and memory usage profiling with large files."
          },
          {
            "id": 2,
            "title": "Develop `feature_engineering.py` for technical indicator generation",
            "description": "Create the `add_features` function to calculate and append 11+ technical indicators to the OHLCV dataframe using `pandas_ta`. This includes log returns, ATR, ROC, RSI, Bollinger Bands, ADX, Stochastic, SMA ratios, and volume metrics, based on a configurable dictionary.",
            "dependencies": [
              1
            ],
            "details": "Implement `feature_engineering.py` and define `add_features(df: pd.DataFrame, indicator_config: dict) -> pd.DataFrame`. Ensure the input DataFrame has a datetime index. Calculate log returns using `ta.log_return`. Dynamically calculate other indicators specified in `indicator_config` using `pandas_ta` (e.g., ATR, ROC, RSI, BBANDS, ADX, STOCH, SMA ratios, volume indicators). Handle `min_periods` for rolling window calculations to avoid excessive NaNs propagating unnecessarily. Leverage vectorized `pandas` operations for performance. Downcast new feature columns to `float32` after calculation, as shown in the provided pseudo-code, to minimize memory footprint.",
            "status": "done",
            "testStrategy": "Property-based tests using `hypothesis` (e.g., `hypothesis.extra.pandas`) to validate indicator calculations across wide ranges of synthetic price data (random walks, trends, volatility changes). Unit tests for specific indicators to ensure correct output against known examples. Verify that `indicator_config` correctly applies and parameterizes indicators. Test NaN handling and downcasting of generated features."
          },
          {
            "id": 3,
            "title": "Create `data_validation.py` for data quality checks and reporting",
            "description": "Develop the `validate_data` function to perform comprehensive data quality checks on processed OHLCV and feature data. This includes verifying required columns, validating data types, handling missing values, detecting outliers, and generating a data quality report.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement `data_validation.py` and define `validate_data(df: pd.DataFrame) -> (pd.DataFrame, dict)`. Verify the presence of all required OHLCV columns (e.g., 'open', 'high', 'low', 'close', 'volume') and the datetime index. Validate data types for all columns (e.g., numerical for prices/features, datetime for index). Identify and handle missing values, applying strategies like forward-fill or dropping rows/columns depending on context. Implement outlier detection using methods such as IQR or Z-score for relevant columns. Log detected issues and generate a data quality report (e.g., dictionary summarizing issues, counts, and proposed actions). Return the cleaned DataFrame and the report.",
            "status": "done",
            "testStrategy": "Unit tests for `validate_data` with dataframes containing various data quality issues: missing OHLCV columns, incorrect data types (e.g., string in 'close'), missing values (NaNs in price, volume), and clearly defined outliers. Verify that the function correctly identifies issues, applies specified handling (fill/drop), and generates an accurate quality report. Test different outlier detection thresholds and reporting formats."
          }
        ]
      },
      {
        "id": 3,
        "title": "Develop Multi-Engine Processing Framework",
        "description": "Implement a flexible `processing_engines` module supporting streaming (Pandas), Dask, and Daft for efficient and scalable processing of various dataset sizes, from medium to very large out-of-core data.",
        "details": "This task involves creating the different processing engines and a factory to select them:\n\n1.  **streaming_engine.py**: Implement `process_streaming` using `pandas.read_csv` with the `chunksize` parameter. Within each chunk, apply `data_processing.add_features` and `data_processing.validate_data`. Aggregate processed chunks into a final `pandas DataFrame`. Focus on minimizing memory usage by explicitly deleting chunks after processing and using `gc.collect()`.\n2.  **dask_engine.py**: Implement `process_dask` using `dask.dataframe` (version 2023.11.0+). Use `dd.read_csv()` to create a lazy Dask DataFrame. Map `data_processing` functions (feature engineering, validation) across partitions using `dask.dataframe.map_partitions` or `apply`. Ensure proper handling of Dask's lazy evaluation and trigger computation with `.compute()`. Configure Dask's scheduler (local or distributed) via `dask.distributed.Client`.\n3.  **daft_engine.py**: Implement `process_daft` leveraging `daft` (version 0.1.2+). Daft is built on Apache Arrow for efficient out-of-core processing. Use `daft.read_csv()` to load data, then apply feature engineering and validation steps using Daft's expression API or `map_batches` for custom Python functions. Emphasize predicate pushdown and columnar operations for performance.\n4.  **index.py (ProcessingEngineFactory)**: Create a factory function (`ProcessingEngineFactory.get_engine`) that selects the appropriate processing engine based on `ProcessingConfig.engine_type` (e.g., 'streaming', 'dask', 'daft') and dataset characteristics (e.g., file size, available RAM).\n\n**Pseudo-code for Engine Selection:**\n```python\nfrom src.utils.config import ProcessingConfig\n\nclass ProcessingEngineFactory:\n    @staticmethod\n    def get_engine(engine_type: str):\n        if engine_type == 'streaming':\n            from .streaming_engine import process_streaming\n            return process_streaming\n        elif engine_type == 'dask':\n            from .dask_engine import process_dask\n            return process_dask\n        elif engine_type == 'daft':\n            from .daft_engine import process_daft\n            return process_daft\n        else:\n            raise ValueError(f'Unknown engine type: {engine_type}')\n\ndef process_with_engine(csv_path: str, config: ProcessingConfig):\n    engine = ProcessingEngineFactory.get_engine(config.engine_type)\n    return engine(csv_path, config)\n```\n\n**Recommended Libraries & Versions:**\n*   `pandas`: 2.1.3+\n*   `dask`: 2023.11.0+ (specifically `dask.dataframe`)\n*   `daft`: 0.1.2+\n*   `pyarrow`: 14.0.1+ (Daft dependency)\n*   `tqdm`: 4.66.1+ (for progress bars in streaming and Dask/Daft workflows)",
        "testStrategy": "Integration tests comparing outputs of all three engines (`streaming_engine`, `dask_engine`, `daft_engine`) for a given input CSV, ensuring identical results. Performance benchmarks for each engine across varying dataset sizes (100MB, 1GB, 10GB+) to validate memory usage and processing time against the 'Process 1GB+ CSV files with <8GB RAM usage' success metric. Unit tests for `ProcessingEngineFactory` to ensure correct engine selection. Critical scenarios: Processing very large files that exceed RAM (triggering Daft or Dask out-of-core mechanisms), memory monitoring within <8GB limit, consistent output formats across engines.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Pandas Streaming Engine (`streaming_engine.py`)",
            "description": "Develop the `process_streaming` function in `streaming_engine.py` using `pandas.read_csv` with `chunksize` for memory-efficient processing. Apply `data_processing.add_features` and `data_processing.validate_data` to each chunk, aggregate results into a final DataFrame, and manage memory effectively.",
            "dependencies": [],
            "details": "Create `src/processing_engines/streaming_engine.py`. Implement `process_streaming(csv_path: str, config: ProcessingConfig)` function. Use `pandas.read_csv(chunksize=...)` to iterate through data. Within each chunk, apply `data_processing.add_features` and `data_processing.validate_data`. Concatenate processed chunks into a final `pandas.DataFrame`. Implement explicit memory cleanup (e.g., `del chunk`, `gc.collect()`) after each chunk to minimize memory usage. Ensure `pandas` version 2.1.3+ is used.",
            "status": "done",
            "testStrategy": "Unit tests for `process_streaming` with small and medium-sized CSVs (e.g., 10MB, 100MB) to verify correct chunk processing, feature addition, data validation, and aggregation. Monitor memory usage during tests to confirm efficient memory management, especially with `chunksize`."
          },
          {
            "id": 2,
            "title": "Implement Dask Processing Engine (`dask_engine.py`)",
            "description": "Develop the `process_dask` function in `dask_engine.py` to leverage Dask DataFrames for lazy, scalable processing of large datasets. Map `data_processing` functions across partitions and manage Dask's computation lifecycle, including scheduler configuration.",
            "dependencies": [],
            "details": "Create `src/processing_engines/dask_engine.py`. Implement `process_dask(csv_path: str, config: ProcessingConfig)` function. Use `dask.dataframe.read_csv()` to create a lazy Dask DataFrame. Apply `data_processing.add_features` and `data_processing.validate_data` using `dask.dataframe.map_partitions` or `apply`. Trigger the computation with `.compute()` to get the final `pandas.DataFrame`. Configure Dask's scheduler (local or distributed) via `dask.distributed.Client` as needed. Ensure `dask` version 2023.11.0+ is used.\n<info added on 2025-10-21T07:42:02.609Z>\n{\n  \"content\": \"Implementation is complete and has been tested. Key features include lazy DataFrame processing using `dask.dataframe.read_csv` with configurable block sizes and automatic partition optimization. The `map_partitions` function is used to apply feature engineering and validation from the `data_processing` module across partitions. The engine supports multiple Dask scheduler configurations (threads, processes, distributed) and includes progress monitoring via `dask.diagnostics.ProgressBar`. Testing confirmed that lazy evaluation works correctly, feature engineering adds 8+ indicators per partition, and data validation achieves 100% quality scores. The engine is fully functional for scalable, out-of-memory dataset processing.\"\n}\n</info added on 2025-10-21T07:42:02.609Z>",
            "status": "done",
            "testStrategy": "Unit tests for `process_dask` with various CSV sizes to verify lazy computation setup, correct mapping of `data_processing` functions across partitions, and accurate final results after `.compute()`. Test with a local Dask client and validate distributed processing logic if applicable."
          },
          {
            "id": 3,
            "title": "Implement Daft Engine (`daft_engine.py`) and Processing Engine Factory (`index.py`)",
            "description": "Develop the `process_daft` function using Daft for Arrow-native, efficient out-of-core processing. Concurrently, create the `ProcessingEngineFactory` in `index.py` to dynamically select the appropriate processing engine (streaming, Dask, or Daft) based on `ProcessingConfig.engine_type`.",
            "dependencies": [
              1,
              2
            ],
            "details": "1. Create `src/processing_engines/daft_engine.py`. Implement `process_daft(csv_path: str, config: ProcessingConfig)` function. Use `daft.read_csv()` to load data and apply `data_processing.add_features` and `data_processing.validate_data` steps using Daft's expression API or `map_batches`. Emphasize predicate pushdown and columnar operations for performance. Ensure `daft` version 0.1.2+ and `pyarrow` 14.0.1+ are used. \n2. Create `src/processing_engines/index.py`. Implement `ProcessingEngineFactory` with a static method `get_engine(engine_type: str)` that imports and returns the respective processing function (`process_streaming`, `process_dask`, `process_daft`) based on `engine_type`. Implement `process_with_engine` as the module's entry point.",
            "status": "done",
            "testStrategy": "Unit tests for `process_daft` validating correct data loading, feature engineering, and validation using Daft's API. Compare Daft's output for small/medium datasets against Pandas/Dask outputs. Unit tests for `ProcessingEngineFactory.get_engine` to ensure it correctly returns the expected engine function for 'streaming', 'dask', and 'daft' types. Integration tests for `process_with_engine` using all three engine types to process a sample CSV, verifying correct engine dispatch and end-to-end data processing."
          }
        ]
      },
      {
        "id": 4,
        "title": "Build HMM Model Training Pipeline",
        "description": "Implement the core HMM model training engine within `model_training` using `hmmlearn`, including configurable parameters, convergence monitoring, and numerical stability handling.",
        "details": "This task focuses on the `hmm_trainer.py` component of the `model_training` module:\n\n1.  **HMM Model Training (`train_model`)**: Implement the function to train `hmmlearn.hmm.GaussianHMM` (version 0.2.8+). It should accept a feature matrix (from `data_processing`), number of states (`n_components`), covariance type (`covariance_type='diag'` as per PRD), maximum iterations (`n_iter`), and `random_state`. Crucially, before training the HMM, apply feature scaling using `sklearn.preprocessing.StandardScaler` or `MinMaxScaler` to normalize input data, which significantly improves HMM convergence and numerical stability. Store the fitted scaler along with the model.\n2.  **Convergence Monitoring**: Configure `hmmlearn` to monitor log-likelihood convergence. Implement logic for multiple restarts (e.g., re-initialize and retrain the model several times with different `random_state`s and pick the one with the highest log-likelihood) to mitigate local optima issues, as HMM training is sensitive to initialization.\n3.  **Numerical Stability**: Implement checks for potential numerical issues (e.g., singular covariance matrices, zero-variance features) and provide warnings or suggest fallbacks (e.g., adding a small epsilon to diagonal of covariance matrix, using `covariance_type='full'` if `diag` fails, or removing problematic features). Validate model parameters (e.g., `n_states` >= 1).\n\n**Pseudo-code for `train_model`:**\n```python\nimport numpy as np\nfrom hmmlearn import hmm\nfrom sklearn.preprocessing import StandardScaler\nfrom src.utils.config import HMMConfig\n\ndef train_model(features: np.ndarray, config: HMMConfig) -> tuple:\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(features)\n\n    best_model = None\n    best_score = -np.inf\n\n    for i in range(config.num_restarts): # Assuming num_restarts in config\n        model = hmm.GaussianHMM(n_components=config.n_states,\n                                covariance_type=config.covariance_type,\n                                n_iter=config.max_iter,\n                                tol=config.tol,\n                                random_state=config.random_state + i, # Vary random state\n                                init_params='stmc', # init startprob, transmat, emissionprob\n                                params='stmc') # update startprob, transmat, emissionprob\n        try:\n            model.fit(scaled_features)\n            score = model.score(scaled_features)\n            if score > best_score:\n                best_score = score\n                best_model = model\n        except Exception as e:\n            print(f\"Warning: HMM training failed for restart {i} with error: {e}\")\n            # Log error, potentially retry or handle numerically unstable cases\n\n    if best_model is None:\n        raise RuntimeError(\"HMM failed to converge after multiple restarts.\")\n\n    return best_model, scaler, best_score\n```\n\n**Recommended Libraries & Versions:**\n*   `hmmlearn`: 0.2.8+\n*   `scikit-learn`: 1.3.2+\n*   `numpy`: 1.26.2+\n*   `scipy`: 1.11.4+ (for statistical functions if needed in numerical stability checks)",
        "testStrategy": "Unit tests for `hmm_trainer.py` to ensure Gaussian HMM trains reliably with different `n_states` (2-5) and `covariance_type`. Test convergence monitoring by verifying log-likelihood increases across iterations. Parameter sensitivity tests by varying `max_iter` and `tol`. Test numerical stability handling with synthetic datasets containing zero-variance features or highly correlated features. Integration tests to ensure features from `data_processing` can be successfully used as input. Critical scenarios: Model failing to converge (should trigger restarts or fallbacks), large number of states, highly volatile input features.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core HMM Training with Feature Scaling",
            "description": "Develop the `train_model` function in `hmm_trainer.py` to instantiate and train `hmmlearn.hmm.GaussianHMM`. Crucially, integrate `sklearn.preprocessing.StandardScaler` to normalize input features before HMM training and ensure the fitted scaler is stored along with the model.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create `src/model_training/hmm_trainer.py`. Implement `train_model` function accepting `features: np.ndarray` and `config: HMMConfig` (from utils). Inside, initialize `StandardScaler`, call `fit_transform` on features. Pass scaled features to `hmm.GaussianHMM` using parameters like `n_components`, `covariance_type='diag'`, `n_iter`, and `random_state` from `config`. The function should return the fitted HMM model and the fitted scaler.",
            "status": "done",
            "testStrategy": "Unit tests for `train_model` ensuring correct HMM instantiation and feature scaling application. Verify that the output includes both the trained model and the fitted scaler. Test with simple synthetic data to confirm basic functionality."
          },
          {
            "id": 2,
            "title": "Enhance HMM Training with Multiple Restarts and Best Model Selection",
            "description": "Extend the HMM training process within `train_model` to include multiple restarts with varying `random_state`s. This is crucial for mitigating local optima issues in HMM training. Implement logic to monitor log-likelihood convergence and select the model instance with the highest log-likelihood across all restarts.",
            "dependencies": [
              1,
              2
            ],
            "details": "Modify the `train_model` function to include a loop for `config.num_restarts`. Inside the loop, vary `random_state` for each `hmm.GaussianHMM` initialization (e.g., `config.random_state + i`). After each `model.fit()`, calculate `model.score(scaled_features)` to get the log-likelihood. Keep track of `best_model` and `best_score` and update them if a higher score is achieved. Handle cases where no model converges successfully across any restart.",
            "status": "done",
            "testStrategy": "Unit tests to verify that `train_model` performs multiple restarts as specified by `config.num_restarts`. Ensure the function correctly identifies and returns the model with the highest log-likelihood. Test with synthetic data known to potentially have multiple local optima to validate best model selection."
          },
          {
            "id": 3,
            "title": "Implement Numerical Stability Checks and Parameter Validation for HMM",
            "description": "Add robust error handling and input validation to the HMM training pipeline. This includes checks for potential numerical issues (e.g., singular covariance matrices, zero-variance features) and providing warnings or fallbacks. Also, validate critical input parameters like `n_states` before model initialization.",
            "dependencies": [
              1,
              2
            ],
            "details": "Before initializing `hmm.GaussianHMM`, validate `config.n_states` (e.g., `n_states >= 1`). Within the training loop (especially around `model.fit()`), add comprehensive `try-except` blocks to catch `ValueError`, `RuntimeError`, or `ConvergenceWarning` from `hmmlearn`. Log informative warnings for issues like singular covariance matrices or zero-variance features. Provide a `RuntimeError` if HMM fails to converge after all restarts. Consider adding a small epsilon to the diagonal of covariance matrices if such customization is necessary for robustness.",
            "status": "done",
            "testStrategy": "Unit tests for parameter validation (e.g., `n_states < 1`). Test with synthetic data designed to cause numerical instability (e.g., constant features leading to zero variance) to ensure warnings or error handling mechanisms are triggered correctly. Verify that the function gracefully handles exceptions from `hmmlearn` during training without crashing."
          }
        ]
      },
      {
        "id": 5,
        "title": "Develop State Inference and Model Persistence",
        "description": "Implement the HMM state inference engine using the Viterbi algorithm and a robust model persistence system for saving and loading trained HMMs and their associated preprocessing parameters.",
        "details": "This task completes the `model_training` module:\n\n1.  **State Inference Engine (`predict_states`)**: Implement the `predict_states` function in `inference_engine.py`. This function will take a trained `hmmlearn.hmm.GaussianHMM` model, a feature matrix, and a fitted `StandardScaler` (obtained during training). It should first scale the input features using the *trained* scaler, then apply `model.predict(scaled_features)` to obtain the hidden state sequence (using the Viterbi algorithm). Also extract state probabilities using `model.predict_proba()`.\n2.  **Lookahead Bias Prevention**: The PRD explicitly mentions 'implement position shifting for realistic inference'. For batch prediction (e.g., on a historical dataset for backtesting), direct use of `model.predict` on all data at once might introduce lookahead bias if the state at `t` is used to make a decision at `t`. The most robust approach for backtesting is to use a lag, meaning a decision at time `t` is based on the inferred state at `t-N` (e.g., `N=1` for a single-period lag). This logic will be primarily handled in the `backtesting` module, but the `inference_engine` should provide the raw states and potentially a helper for lagged states.\n3.  **Model Persistence (`save_model`, `load_model`)**: Implement `save_model` and `load_model` functions in `model_persistence.py`. As per the PRD, use `pickle` (built-in module) for serialization. Store a dictionary containing the trained HMM model, the feature scaler, and any relevant `HMMConfig` or metadata (e.g., `model_version`, `training_date`, `feature_names`). This ensures all necessary components are saved and loaded consistently. Add integrity validation on load (e.g., check if expected keys exist in the loaded dictionary, perform a basic prediction test).\n\n**Security Note on Pickle:** While `pickle` is convenient for Python objects, it is not secure against maliciously constructed data. Never unpickle data received from an untrusted or unauthenticated source, as it can execute arbitrary code. For this project, assuming internal use or trusted data sources, it adheres to the PRD requirement. If external sharing were a goal, `joblib` or ONNX would be alternatives.\n\n**Pseudo-code for `save_model`:**\n```python\nimport pickle\nimport os\nfrom hmmlearn import hmm\nfrom sklearn.preprocessing import StandardScaler\nfrom src.utils.config import HMMConfig\n\ndef save_model(model: hmm.GaussianHMM, scaler: StandardScaler, config: HMMConfig, path: str):\n    model_data = {\n        'hmm_model': model,\n        'scaler': scaler,\n        'config': config.dict(), # Convert Pydantic config to dict for pickling\n        'version': '1.0',\n        'timestamp': pd.Timestamp.now().isoformat()\n    }\n    with open(path, 'wb') as f:\n        pickle.dump(model_data, f)\n\ndef load_model(path: str) -> tuple:\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Model file not found at {path}\")\n    with open(path, 'rb') as f:\n        model_data = pickle.load(f)\n    \n    # Basic integrity validation\n    if not all(k in model_data for k in ['hmm_model', 'scaler', 'config']):\n        raise ValueError(\"Corrupted model file: missing essential components.\")\n    \n    # Reconstruct Pydantic config after loading\n    config = HMMConfig(**model_data['config'])\n    return model_data['hmm_model'], model_data['scaler'], config\n```\n\n**Recommended Libraries & Versions:**\n*   `hmmlearn`: 0.2.8+\n*   `scikit-learn`: 1.3.2+\n*   `numpy`: 1.26.2+\n*   `pandas`: 2.1.3+ (for Timestamp)",
        "testStrategy": "Unit tests for `inference_engine.py` to verify accurate state prediction against known HMM sequences and correct application of feature scaling. Test `predict_proba` for probability distribution correctness. Unit tests for `model_persistence.py` to ensure successful save/load operations for various model configurations and sizes. Test integrity validation (e.g., loading a tampered file). Integration tests to ensure a trained model can be saved, reloaded, and then used for inference, yielding identical results. Critical scenarios: Model loaded from disk predicts correctly, handling of large models during serialization, validation of integrity on load.",
        "priority": "medium",
        "dependencies": [
          1,
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement HMM State Inference Engine Core Functionality",
            "description": "Develop the `predict_states` function in `inference_engine.py`. This includes scaling input features using a provided `StandardScaler` and then applying the trained `hmmlearn.hmm.GaussianHMM` model to predict hidden state sequences and their probabilities.",
            "dependencies": [],
            "details": "Implement `predict_states(model, scaler, features)` which first `scaler.transform(features)` then `model.predict(scaled_features)` for the state sequence. Additionally, obtain state probabilities using `model.predict_proba(scaled_features)`. Ensure the function returns both the inferred state sequence and the probability matrix. Leverage `hmmlearn.hmm.GaussianHMM` and `sklearn.preprocessing.StandardScaler` for the implementation.",
            "status": "done",
            "testStrategy": "Unit tests for `inference_engine.py` to verify accurate state prediction against known HMM sequences and correct application of feature scaling. Test `predict_proba` for probability distribution correctness (e.g., rows sum to 1)."
          },
          {
            "id": 2,
            "title": "Develop HMM Model Persistence and Loading with Integrity Validation",
            "description": "Create `save_model` and `load_model` functions in `model_persistence.py` using Python's `pickle` module. These functions will serialize and deserialize trained HMM models, their associated feature scalers, and HMM configuration, including robust integrity validation upon loading.",
            "dependencies": [],
            "details": "Implement `save_model(model, scaler, config, path)` and `load_model(path)` as per the provided pseudo-code. The `save_model` function must store a dictionary containing `'hmm_model'`, `'scaler'`, `'config'` (converted to dict), `'version'`, and `'timestamp'`. The `load_model` function must include checks for existence of expected keys (`'hmm_model'`, `'scaler'`, `'config'`) in the loaded dictionary to validate integrity and raise `ValueError` for corrupted files. Reconstruct `HMMConfig` from the loaded dictionary.",
            "status": "done",
            "testStrategy": "Unit tests for `model_persistence.py` to ensure successful save/load operations for various valid combinations of HMMs, scalers, and configs. Test error handling for non-existent files and corrupted (missing key) model files. Perform a basic prediction test after loading to ensure functional integrity of the loaded components."
          },
          {
            "id": 3,
            "title": "Integrate Lagged State Retrieval Support in Inference Engine",
            "description": "Enhance the `inference_engine.py` to explicitly support the retrieval of lagged hidden states. This is crucial for preventing lookahead bias in backtesting by ensuring decisions are based on past information. This might involve a helper function or modification of the `predict_states` output for easy consumption by a lagging mechanism.",
            "dependencies": [
              1
            ],
            "details": "Implement a helper function like `get_lagged_states(raw_states, lag_periods)` within `inference_engine.py`. This function should take the raw state sequence generated by `predict_states` and return a new sequence where each state at time `t` corresponds to the inferred state at `t-N` (e.g., `N=1` for a single-period lag). Ensure proper handling of initial periods where a lag cannot be applied (e.g., by padding with a designated 'no-state' value or a default state).",
            "status": "done",
            "testStrategy": "Unit tests for the new `get_lagged_states` helper function in `inference_engine.py`, verifying correct state shifts and handling of `lag_periods`. Test with different lag values and edge cases (e.g., `lag_periods = 0`, `lag_periods >= data_length`)."
          }
        ]
      },
      {
        "id": 6,
        "title": "Create Regime-Based Backtesting Engine",
        "description": "Develop the core `backtesting` module's strategy engine to implement state-based position allocation, mapping HMM-inferred states to trading positions (long/short/flat) with realistic trade execution.",
        "details": "This task covers the `strategy_engine.py` component:\n\n1.  **Regime-Based Position Allocation (`backtest_strategy`)**: Implement a function that takes the HMM state sequence (from `model_training.inference_engine`), price data (from `data_processing`), and a strategy configuration (from `utils.config`). This configuration will define the mapping from specific HMM states to trading actions (e.g., State 0 = Long, State 1 = Short, State 2 = Flat). The engine should iterate through the time series, applying the state-to-position mapping to generate a `position_array` (e.g., 1 for long, -1 for short, 0 for flat).\n2.  **Position Transitions**: Implement realistic handling of position transitions, including transaction costs (slippage, commissions) if specified in the config, and avoiding over-optimization. Ensure that trades are only executed at the open of the next bar *after* a state change is identified, respecting the lookahead bias prevention principles. Vectorize calculations where possible to optimize performance for large datasets.\n3.  **Output**: The function should return a `position_array` and a `trade_log` (list of `Trade` objects, potentially defined in `utils.data_types`) detailing entry/exit points, prices, and P&L.\n\n**Pseudo-code for `backtest_strategy` (simplified):**\n```python\nimport pandas as pd\nimport numpy as np\nfrom src.utils.data_types import BacktestConfig # Assume a BacktestConfig with state_map and fees\n\ndef backtest_strategy(states: np.ndarray, prices: pd.Series, config: BacktestConfig) -> tuple:\n    positions = np.zeros_like(states, dtype=int)\n    trades = [] # List of Trade objects\n    \n    current_position = 0\n    entry_price = np.nan\n\n    # Note: Lookahead bias prevention is crucial here. States should be lagged.\n    # For simplicity here, assume states are already appropriately lagged/forward-shifted\n    # e.g. states[t-1] is used for decisions at time t.\n\n    for i in range(1, len(states)): # Start from 1 to look at previous state\n        current_state = states[i-1] # Decision at `i` is based on state at `i-1`\n        target_position = config.state_map.get(current_state, 0)\n        \n        if target_position != current_position:\n            # Place trade at current price (prices[i] could be Open or Close depending on strategy)\n            # For simplicity, let's assume decision at end of day t-1, executed at open of day t\n            trade_price = prices.iloc[i]\n            # Simulate transaction costs here\n            # ...\n\n            # Log trade, update position\n            trades.append(Trade(time=prices.index[i], action='BUY' if target_position > current_position else 'SELL', ...))\n            current_position = target_position\n            # ... handle entry/exit prices, P&L\n\n        positions[i] = current_position\n\n    # Post-process positions and trades into a clean DataFrame or object\n    return pd.Series(positions, index=prices.index), trades\n```\n\n**Recommended Libraries & Versions:**\n*   `pandas`: 2.1.3+\n*   `numpy`: 1.26.2+\n*   Custom classes from `utils.data_types`",
        "testStrategy": "Unit tests for `strategy_engine.py` using synthetic price data and predetermined state sequences to verify correct position generation and trade logging. Test edge cases such as continuous states, rapid state changes, and zero-volume periods. Validate state-to-position mapping accuracy against configured rules. Integration tests to confirm that HMM-inferred states can drive strategy execution correctly. Critical scenarios: Rapid entry/exit, handling of transaction costs, correct position sizing based on config, ensure no lookahead bias in position generation by explicit lagging.",
        "priority": "medium",
        "dependencies": [
          1,
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core Regime-Based Position Mapping with Lookahead Prevention",
            "description": "Develop the initial `backtest_strategy` function within `strategy_engine.py` to map HMM-inferred states to target positions (long/short/flat) based on `config.state_map`. Ensure strict lookahead bias prevention by using lagged states for all trading decisions. The output should be a raw `position_array` indicating target positions without immediate trade execution.",
            "dependencies": [],
            "details": "Create the `backtest_strategy` function signature: `backtest_strategy(states: np.ndarray, prices: pd.Series, config: BacktestConfig) -> pd.Series`. Implement the core loop that iterates through the time series. For each time step `i`, determine the target position using `states[i-1]` and `config.state_map`. Populate a `positions` pandas Series or numpy array (1 for long, -1 for short, 0 for flat). Ensure `BacktestConfig` and `Trade` dataclasses are available from `utils.data_types` and `config` module.",
            "status": "done",
            "testStrategy": "Unit tests using synthetic price data and pre-defined HMM state sequences. Verify that the `position_array` correctly reflects the `state_map` for each time step, explicitly validating lookahead prevention by asserting that decisions at time `t` are based solely on information available at `t-1` or earlier. Test cases should include different `state_map` configurations, continuous states, and rapid state changes."
          },
          {
            "id": 2,
            "title": "Develop Realistic Trade Execution & Transaction Cost Modeling",
            "description": "Enhance the `backtest_strategy` function to simulate realistic trade execution by processing position changes identified in Subtask 1. This involves executing trades only at the open of the next bar *after* a state change, incorporating transaction costs (slippage, commissions) from the configuration, and accurately managing the `current_position` over time.",
            "dependencies": [
              1
            ],
            "details": "Modify the `backtest_strategy` to track the `current_position`. When a change from `current_position` to `target_position` (from subtask 1) is detected, simulate a trade at the current bar's open price (`prices.iloc[i]`). Calculate and apply transaction costs using `config.slippage_bps` (as a percentage of trade value) and `config.commission_per_trade` (fixed cost per trade). Update `current_position` after each simulated trade. Ensure vectorized calculations are used where possible for cost application to optimize performance.",
            "status": "done",
            "testStrategy": "Unit tests with synthetic data including various state change patterns (e.g., long-flat-short, multiple consecutive long positions). Verify correct trade execution timing (decision at `t-1`, execution at `t`). Validate transaction cost application for both entry and exit trades. Test scenarios with varying `slippage_bps` and `commission_per_trade` values (including zero costs) to confirm accurate P&L impact."
          },
          {
            "id": 3,
            "title": "Implement Trade Logging and Final Output Generation",
            "description": "Finalize the `backtest_strategy` function to meticulously log all executed trades into a `trade_log` (a list of `Trade` objects, defined in `utils.data_types`) and return both the final `position_array` and the comprehensive `trade_log`. Each `Trade` object must capture critical details such as entry/exit points, prices, P&L, and transaction costs.",
            "dependencies": [
              2
            ],
            "details": "Define a `Trade` dataclass in `src.utils.data_types` with attributes like `entry_time`, `entry_price`, `exit_time`, `exit_price`, `size`, `pnl`, `commission`, `slippage`. Within `backtest_strategy`, populate a list of these `Trade` objects during the trade execution phase (Subtask 2). Calculate the final P&L for each trade, including all costs. The function should ultimately return a `tuple` containing the `pd.Series` representing the final `position_array` and the `list[Trade]` representing the `trade_log`.",
            "status": "done",
            "testStrategy": "Unit tests to verify that the returned `trade_log` contains accurate and complete `Trade` objects for all simulated trades. Validate the P&L calculations within each `Trade` object, ensuring transaction costs are correctly reflected. Confirm that the final `position_array` is properly aligned with the `prices.index` and accurately reflects the state after all transitions and trade executions. Test edge cases like no trades occurring."
          }
        ]
      },
      {
        "id": 7,
        "title": "Develop Backtest Performance Metrics & Bias Prevention",
        "description": "Implement comprehensive performance metric calculation for strategy evaluation and build robust lookahead bias detection and prevention mechanisms within the `backtesting` module.",
        "details": "This task complements the backtesting module with analytics and validation:\n\n1.  **Performance Metrics Calculation (`calculate_performance`)**: Implement this function in `performance_metrics.py`. It should take an equity curve `pd.Series` (generated by `strategy_engine`), optional benchmark data, and a risk-free rate. Calculate comprehensive risk-adjusted performance metrics, including:\n    *   Annualized Returns (CAGR)\n    *   Annualized Volatility\n    *   Sharpe Ratio (using `numpy` and `pandas` for calculations, as `empyrical` or `quantstats` are alternatives but PRD implies custom logic)\n    *   Maximum Drawdown and Drawdown Duration\n    *   Calmar Ratio\n    *   Win Rate, Loss Rate, Profit Factor\n    *   Sortino Ratio (if applicable)\n    Ensure intraday data scaling is handled correctly for annualization. The `PerformanceMetrics` dataclass from `utils.data_types` should store these results.\n2.  **Lookahead Bias Prevention (`prevent_lookahead`)**: Implement this logic in `bias_prevention.py`. While the core strategy engine in Task 6 should implement lagging, this module provides explicit functions to detect and report potential bias sources. This could involve:\n    *   **Timing Consistency Check**: Verify that `state[t]` is not used to make decisions at `t` or earlier. This is primarily a verification step.\n    *   **Data Availability Validation**: Ensure that all features used for HMM inference at time `t` were genuinely available at `t` (e.g., no 'future' close prices used for indicator calculation). This requires careful data pipeline design in `data_processing`.\n    *   **Position Shifting Validation**: Confirm that if `states[t-N]` is used, the resulting performance is indeed bias-free by comparing against a benchmark where known bias exists. This feature could also provide a utility to explicitly shift state sequences by a specified lag `N` for users if needed.\n\n**Pseudo-code for Sharpe Ratio:**\n```python\nimport numpy as np\nimport pandas as pd\n\ndef calculate_sharpe_ratio(equity_curve: pd.Series, risk_free_rate: float = 0.02) -> float:\n    daily_returns = equity_curve.pct_change().dropna()\n    if daily_returns.empty:\n        return 0.0\n\n    # Annualization factor depends on data frequency\n    # Assuming daily data (252 trading days)\n    annualization_factor = 252.0 # Can be configured based on data frequency (e.g., 252 for daily, 52 for weekly, etc.)\n\n    excess_returns = daily_returns - (risk_free_rate / annualization_factor)\n    sharpe = np.sqrt(annualization_factor) * excess_returns.mean() / excess_returns.std()\n    return sharpe\n```\n\n**Recommended Libraries & Versions:**\n*   `pandas`: 2.1.3+\n*   `numpy`: 1.26.2+\n*   Custom classes from `utils.data_types`",
        "testStrategy": "Unit tests for `performance_metrics.py` to calculate metrics (Sharpe, Max Drawdown) against manually computed results for various synthetic equity curves. Test edge cases (flat equity curve, all losses). Unit tests for `bias_prevention.py` to validate timing consistency checks and report potential bias sources using specially crafted datasets. Integration tests to ensure backtest results flow into performance calculations and bias prevention validation correctly. Critical scenarios: Accurate Sharpe ratio for different annualization periods, correct identification of lookahead bias, robust handling of zero-volatility returns.",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core Risk-Adjusted Performance Metrics",
            "description": "Develop the initial `calculate_performance` function in `performance_metrics.py` to compute fundamental risk-adjusted performance metrics. This includes Annualized Returns (CAGR), Annualized Volatility, Sharpe Ratio, and Maximum Drawdown.",
            "dependencies": [
              6
            ],
            "details": "Implement the `calculate_performance` function that takes an equity curve (pd.Series), optional benchmark, and risk-free rate. Focus on calculating CAGR, Annualized Volatility, Sharpe Ratio (using provided pseudo-code with numpy/pandas), and Maximum Drawdown and its duration. Ensure correct annualization factors are applied based on data frequency (e.g., 252 for daily).",
            "status": "done",
            "testStrategy": "Unit tests for `performance_metrics.py` to calculate CAGR, Volatility, Sharpe Ratio, and Maximum Drawdown against manually computed results for various synthetic equity curves. Test edge cases such as flat equity curves or curves with only losses."
          },
          {
            "id": 2,
            "title": "Implement Advanced Performance Metrics & Dataclass Integration",
            "description": "Extend the `calculate_performance` function to include additional advanced metrics such as Calmar Ratio, Win Rate, Loss Rate, Profit Factor, and Sortino Ratio. Integrate all computed results into the `PerformanceMetrics` dataclass.",
            "dependencies": [
              1
            ],
            "details": "Build upon the work in Subtask 1. Add calculation logic for Calmar Ratio, Win Rate, Loss Rate, Profit Factor, and Sortino Ratio. Ensure all metrics, both from Subtask 1 and this subtask, are correctly populated into the `PerformanceMetrics` dataclass from `utils.data_types`. Handle scenarios like zero standard deviation for Sortino Ratio. Confirm intraday data scaling is handled for annualization.",
            "status": "done",
            "testStrategy": "Unit tests to verify the accuracy of newly implemented metrics (Calmar Ratio, Win/Loss Rate, Profit Factor, Sortino Ratio). Integration tests to ensure the `PerformanceMetrics` dataclass is fully and correctly populated with all calculated values from various equity curve scenarios."
          },
          {
            "id": 3,
            "title": "Develop Lookahead Bias Detection and Prevention Utilities",
            "description": "Implement the `bias_prevention.py` module, focusing on creating explicit functions to detect and report potential sources of lookahead bias, including timing consistency checks and data availability validation.",
            "dependencies": [
              2,
              6
            ],
            "details": "Create functions within `bias_prevention.py` for 'Timing Consistency Check' to verify decisions are made only with available past data, and 'Data Availability Validation' to ensure features used were genuinely available at `t`. Optionally, develop 'Position Shifting Validation' logic that can compare performance with known bias against a shifted version. This module should provide detection and reporting, complementing lagging logic in the strategy engine.",
            "status": "done",
            "testStrategy": "Unit tests for `bias_prevention.py` using synthetic datasets specifically designed to exhibit lookahead bias (e.g., using future data for indicator calculation) to ensure the detection mechanisms trigger correctly. Test the reporting functionality and any utility functions for shifting state sequences."
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement Visualization & Reporting Module",
        "description": "Develop the `visualization` module to generate publication-ready charts of HMM states overlaid on price data, interactive performance dashboards, and detailed regime analysis reports.",
        "details": "This task implements the `visualization` module:\n\n1.  **State Visualization Engine (`plot_states`)**: Implement this in `chart_generator.py`. Use `matplotlib` (version 3.8.2+) for static charts. Create multi-panel plots showing OHLCV data, HMM states (color-coded, overlaid on price charts), and selected technical indicators (from `data_processing`). Leverage `mplfinance` (version 0.12.9b0+) for efficient candlestick chart generation with overlays. Ensure customizable time ranges, legends, and annotations. Output to publication-ready formats (PNG/SVG).\n2.  **Performance Dashboard Generation (`build_dashboard`)**: Implement this in `dashboard_builder.py`. Use `plotly` (version 5.18.0+) to create interactive HTML dashboards. The dashboard should display key performance metrics (from `backtesting.performance_metrics`), interactive equity curves, drawdown charts, and potentially pie/bar charts for regime durations. The output should be a single standalone HTML file with interactive elements.\n3.  **Regime Analysis Reports (`generate_regime_report`)**: Implement this in `report_generator.py`. Generate PDF/HTML reports detailing regime characteristics (e.g., mean return, volatility per state), transition probabilities (using `hmm.transmat_`), and feature distributions within each state. Use `jinja2` (version 3.1.2+) for templating the report content and `WeasyPrint` (version 61.1+) to convert HTML to PDF if PDF output is required, or just output HTML directly.\n\n**Pseudo-code for `plot_states`:**\n```python\nimport mplfinance as mpf\nimport pandas as pd\nimport numpy as np\n\ndef plot_states(prices: pd.DataFrame, states: np.ndarray, indicators: pd.DataFrame, config: dict):\n    # Create a DataFrame for plotting, ensure alignment\n    plot_df = prices.copy()\n    plot_df['HMM_State'] = states # Ensure states are mapped to meaningful labels/colors\n\n    # Example: create an 'apds' list for mplfinance to overlay states as colored bands\n    apds = []\n    unique_states = np.unique(states)\n    colors = ['blue', 'green', 'red', 'purple', 'orange'] # Map states to colors\n    for s_val in unique_states:\n        state_mask = (plot_df['HMM_State'] == s_val)\n        # Create a series for the state, fill with nan where not in state\n        state_band = plot_df['close'].copy()\n        state_band[~state_mask] = np.nan\n        \n        apds.append(mpf.make_addplot(state_band, type='hollow_candles', color=colors[s_val], width=0.7,\n                                     panel=0, alpha=0.3, scatter=False))\n\n    # Overlay indicators\n    # Add more apds for indicators as needed\n    # mpf.make_addplot(indicators['RSI'], panel=1, color='orange')\n    \n    # Generate plot\n    mpf.plot(plot_df, type='candle', style='yahoo', volume=True, addplot=apds, \n             title='HMM States Overlayed on Price Data', figscale=1.5, savefig='hmm_states.png')\n```\n\n**Recommended Libraries & Versions:**\n*   `matplotlib`: 3.8.2+\n*   `mplfinance`: 0.12.9b0+\n*   `plotly`: 5.18.0+\n*   `jinja2`: 3.1.2+\n*   `WeasyPrint`: 61.1+ (for PDF reports)\n*   `pandas`: 2.1.3+\n*   `numpy`: 1.26.2+",
        "testStrategy": "Unit tests for `chart_generator.py` to ensure charts render without errors and contain expected elements (legends, annotations, correctly overlaid states and indicators) using mock data. Visual validation is key here. Unit tests for `dashboard_builder.py` to confirm interactive elements function correctly and all metrics are displayed accurately in a generated HTML file. Unit tests for `report_generator.py` to verify report content and structure (e.g., correct tables, charts embedded) using templating. Integration tests to ensure data from `backtesting`, `model_training`, and `data_processing` can be correctly visualized and reported. Critical scenarios: Large datasets rendering efficiently, responsive dashboard elements, export functionality.",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          5,
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement HMM State Visualization Engine (`plot_states`)",
            "description": "Develop the `plot_states` function in `chart_generator.py` to generate static, publication-ready charts. These charts will overlay HMM states (color-coded) on OHLCV price data and selected technical indicators from the `data_processing` module.",
            "dependencies": [
              2
            ],
            "details": "Implement `plot_states` in `chart_generator.py`. Utilize `matplotlib` (3.8.2+) and `mplfinance` (0.12.9b0+) for efficient candlestick chart generation with state overlays. Ensure customizable time ranges, interactive legends, and annotations. Output must be to publication-ready formats like PNG and SVG. Integrate with `data_processing` for technical indicator data.",
            "status": "done",
            "testStrategy": "Unit tests for `chart_generator.py` to verify charts render without errors, displaying correct OHLCV data, HMM state overlays, and indicators. Test different time ranges, number of states, and output formats (PNG/SVG). Visual inspection is crucial for correctness and aesthetic appeal."
          },
          {
            "id": 2,
            "title": "Develop Interactive Performance Dashboard (`build_dashboard`)",
            "description": "Create the `build_dashboard` function in `dashboard_builder.py` to generate interactive HTML dashboards. These dashboards will display key backtesting performance metrics, interactive equity curves, drawdown charts, and regime duration analysis.",
            "dependencies": [
              7
            ],
            "details": "Implement `build_dashboard` in `dashboard_builder.py`. Use `plotly` (5.18.0+) to create interactive HTML dashboards. Integrate performance metrics from `backtesting.performance_metrics`. The dashboard should include interactive equity curves, drawdown charts, and visual representations (e.g., pie/bar charts) of regime durations. The final output must be a single standalone HTML file with all interactive elements.",
            "status": "done",
            "testStrategy": "Unit tests for `dashboard_builder.py` to ensure interactive elements function as expected, performance metrics are displayed correctly, and the HTML output is valid and self-contained. Test with various backtest results, including edge cases like flat equity curves or scenarios with high drawdowns."
          },
          {
            "id": 3,
            "title": "Implement Detailed Regime Analysis Report Generator (`generate_regime_report`)",
            "description": "Develop the `generate_regime_report` function in `report_generator.py` to produce detailed PDF/HTML reports. These reports will describe HMM regime characteristics, transition probabilities (from `hmm.transmat_`), and feature distributions within each state.",
            "dependencies": [],
            "details": "Implement `generate_regime_report` in `report_generator.py`. Utilize `jinja2` (3.1.2+) for templating the report content. Include details on mean return, volatility per state, transition probabilities (using `hmm.transmat_`), and feature distributions within each state. Use `WeasyPrint` (61.1+) for converting templated HTML to PDF if PDF output is required, otherwise directly output HTML. Ensure the reports are well-formatted and easy to read.",
            "status": "done",
            "testStrategy": "Unit tests for `report_generator.py` to verify report content accuracy, correct formatting, and successful generation of both HTML and PDF outputs. Test with different HMM model outputs to ensure all regime characteristics and transition probabilities are correctly captured and presented."
          }
        ]
      },
      {
        "id": 9,
        "title": "Build CLI Integration and Orchestration",
        "description": "Implement a comprehensive command-line interface (`cli.py`) to orchestrate all modules, providing a single-command execution pipeline for HMM futures analysis, along with robust error handling and performance optimizations.",
        "details": "This task integrates all previously developed modules into a user-friendly CLI tool:\n\n1.  **CLI Interface (`cli.py`)**: Use `Click` (version 8.1.7+) to build the command-line interface. Define top-level commands (e.g., `analyze`, `train`, `backtest`, `visualize`) and subcommands/options for all configurable parameters (e.g., input CSV path, HMM states, processing engine type, output directory). The CLI should orchestrate the entire analysis pipeline, calling functions from `data_processing`, `processing_engines`, `model_training`, `backtesting`, and `visualization` in the correct sequence based on the dependency graph.\n2.  **Comprehensive Error Handling**: Integrate `utils.logging_config` for detailed logging. Implement global error handling within the CLI using `try-except` blocks. Provide clear, user-friendly error messages with suggested fixes, distinct from technical stack traces (which can be logged at DEBUG level). Leverage `Click`'s error handling features.\n3.  **Performance Optimization & Memory Management**: Integrate `tqdm` (version 4.66.1+) for progress bars during long-running operations (e.g., data processing, HMM training). Continuously monitor memory usage and provide warnings if limits are approached, leveraging Python's `resource` module or `psutil` (version 5.9.6+) for system metrics. Ensure efficient data passing between modules to avoid unnecessary copying. The multi-engine processing framework (Task 3) will be dynamically selected here based on input parameters and estimated data size.\n\n**Pseudo-code for CLI structure (using Click):**\n```python\nimport click\nimport os\nfrom src.utils.config import HMMConfig, ProcessingConfig\nfrom src.data_processing.index import DataProcessor\nfrom src.processing_engines.index import ProcessingEngineFactory\nfrom src.model_training.index import HMMTrainer\nfrom src.backtesting.index import BacktestEngine\nfrom src.visualization.index import VisualizationEngine\nfrom src.utils.logging_config import setup_logging\n\n@click.group()\ndef cli():\n    pass\n\n@cli.command()\n@click.option('--input-csv', type=click.Path(exists=True), required=True, help='Path to input OHLCV CSV file.')\n@click.option('--output-dir', type=click.Path(), default='./results', help='Output directory for results.')\n@click.option('--n-states', type=int, default=3, help='Number of hidden states for HMM.')\n@click.option('--engine', type=str, default='streaming', help='Processing engine (streaming, dask, daft).')\n@click.option('--log-level', type=click.Choice(['DEBUG', 'INFO', 'WARNING', 'ERROR']), default='INFO', help='Logging level.')\ndef analyze(\n    input_csv, output_dir, n_states, engine, log_level\n):\n    setup_logging(log_level)\n    click.echo(f'Starting HMM analysis for {input_csv}...')\n    os.makedirs(output_dir, exist_ok=True)\n\n    try:\n        # 1. Data Processing\n        processing_config = ProcessingConfig(engine_type=engine, indicators={'sma': {'length': 20}})\n        processed_data = ProcessingEngineFactory.get_engine(engine)(input_csv, processing_config)\n        click.echo('Data processing complete.')\n\n        # 2. HMM Training\n        hmm_config = HMMConfig(n_states=n_states)\n        trainer = HMMTrainer()\n        model, scaler, _ = trainer.train_model(processed_data.drop(columns=['open', 'high', 'low', 'volume']), hmm_config)\n        click.echo('HMM training complete.')\n        \n        # 3. Inference and Persistence (simplified)\n        states, _ = trainer.predict_states(model, scaler, processed_data.drop(columns=['open', 'high', 'low', 'volume']))\n        trainer.save_model(model, scaler, os.path.join(output_dir, 'hmm_model.pkl'))\n\n        # 4. Backtesting (simplified)\n        backtest_engine = BacktestEngine()\n        equity_curve, _ = backtest_engine.backtest_strategy(states, processed_data['close'], None) # Need strategy config\n        metrics = backtest_engine.calculate_performance(equity_curve)\n        click.echo('Backtesting complete.')\n\n        # 5. Visualization (simplified)\n        vis_engine = VisualizationEngine()\n        vis_engine.plot_states(processed_data[['open','high','low','close']], states, processed_data.drop(columns=['open','high','low','close','volume']), os.path.join(output_dir, 'states_chart.png'))\n        vis_engine.build_dashboard(metrics, os.path.join(output_dir, 'dashboard.html'))\n        click.echo(f'Results saved to {output_dir}')\n\n    except Exception as e:\n        click.echo(f'Error during analysis: {e}', err=True)\n        click.echo('Please check the log for details.', err=True)\n        # Log full traceback at DEBUG level\n\nif __name__ == '__main__':\n    cli() # pragma: no cover\n```\n\n**Recommended Libraries & Versions:**\n*   `Click`: 8.1.7+\n*   `tqdm`: 4.66.1+\n*   `psutil`: 5.9.6+ (optional, for advanced memory monitoring)",
        "testStrategy": "End-to-end integration tests for `cli.py` covering various command combinations, input parameters, and expected outputs. Test error cases by providing invalid inputs (e.g., non-existent file, invalid HMM states) and verify correct error messages are displayed and logged. Performance benchmarking for the complete pipeline to ensure processing of 1GB+ CSVs with <8GB RAM and model convergence <5 minutes. Critical scenarios: Full pipeline execution with Dask/Daft, graceful handling of unexpected module failures, accurate progress reporting, consistent output artifact generation.",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3,
          5,
          7,
          8
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core CLI Structure and Analysis Command",
            "description": "Develop the `cli.py` entry point using Click, define the `analyze` command, and integrate options for `input-csv`, `output-dir`, `n-states`, `engine`, and `log-level`. Orchestrate calls to `data_processing`, `processing_engines`, `model_training`, `backtesting`, and `visualization` modules in sequence as outlined in the pseudo-code.",
            "dependencies": [
              1,
              2,
              3,
              5,
              7,
              8
            ],
            "details": "Use `Click` (version 8.1.7+) to create the `@click.group()` and `@cli.command('analyze')`. Implement all specified `@click.option` decorators with appropriate types (e.g., `click.Path`, `click.Choice`) and descriptive help messages. Ensure the `analyze` function properly calls `setup_logging` from `utils.logging_config` and `os.makedirs` for the output directory. Follow the provided pseudo-code structure to instantiate and call functions from `DataProcessor`, `ProcessingEngineFactory`, `HMMTrainer`, `BacktestEngine`, and `VisualizationEngine` using the parameters passed via CLI options. Focus on correct module import paths.",
            "status": "done",
            "testStrategy": "Write end-to-end integration tests for `cli.py` to invoke the `analyze` command with various valid parameters (e.g., different input CSVs, number of states, engines, log levels). Verify that output directories are created, and that the expected sequence of module functions is called (using mocks for lower-level module functions if necessary to isolate CLI logic). Assert successful execution and no unhandled exceptions for valid inputs."
          },
          {
            "id": 2,
            "title": "Integrate Comprehensive Error Handling and Detailed Logging",
            "description": "Integrate `utils.logging_config` to establish a robust logging system within the CLI. Implement global `try-except` blocks around the main analysis pipeline within the `analyze` command to catch exceptions, display clear, user-friendly error messages to the console, and log full technical stack traces at a DEBUG level for diagnostics.",
            "dependencies": [
              1,
              1
            ],
            "details": "Modify the `analyze` command in `cli.py` to wrap the entire analysis logic (from data processing through visualization) in a `try-except Exception as e:` block. Use `click.echo(f'Error during analysis: {e}', err=True)` for user-facing error messages on `stderr` and `click.echo('Please check the log for details.', err=True)`. Ensure `setup_logging(log_level)` from `utils.logging_config` is called first. Within the `except` block, log the full traceback using `logger.exception('An unexpected error occurred during CLI execution.')` at DEBUG level, preventing raw stack traces from appearing in user-facing output.",
            "status": "done",
            "testStrategy": "Develop unit and integration tests that simulate various error conditions (e.g., non-existent input CSV, invalid configuration passed to internal modules, module-specific exceptions during data processing or training). Assert that user-friendly error messages are printed to `stderr`, and that detailed stack traces are logged correctly at DEBUG level without being displayed to the user on the console. Verify `log_level` option correctly filters log outputs."
          },
          {
            "id": 3,
            "title": "Implement Performance Optimizations with Progress Bars and Memory Monitoring",
            "description": "Integrate `tqdm` progress bars for long-running operations (e.g., data processing, HMM training) within the `analyze` command to provide user feedback. Implement memory usage monitoring using `psutil` or Python's `resource` module, providing warnings through the logging system if memory limits are approached during execution, and ensure efficient data passing between modules.",
            "dependencies": [
              1,
              2
            ],
            "details": "Incorporate `tqdm` (version 4.66.1+) around iterative or data-intensive steps in `cli.py`. For example, wrap `DataProcessor` or `HMMTrainer` calls if they expose an iterable or a progress callback. Implement periodic memory checks using `psutil` (version 5.9.6+). Define a configurable memory threshold (e.g., as a percentage of available RAM or a fixed limit) in `utils.config` and issue log warnings (e.g., `logger.warning`) when this threshold is exceeded. Focus on ensuring data structures are passed by reference where appropriate and unnecessary copies are avoided between modules (e.g., `processed_data` between processing and training).",
            "status": "done",
            "testStrategy": "Test with large synthetic datasets to verify `tqdm` progress bars display correctly, update in real-time, and complete without errors. Simulate high memory usage conditions (e.g., by loading extremely large datasets or creating memory-intensive objects) to confirm memory monitoring triggers warnings in the logs as expected. Monitor system resources (CPU, RAM) during CLI execution tests to validate the `psutil` integration works and that memory usage is within reasonable bounds given the input data. Verify that the selected processing engine (streaming, dask, daft) handles data efficiently."
          }
        ]
      },
      {
        "id": 10,
        "title": "Comprehensive Testing, Documentation & Examples",
        "description": "Finalize the project by implementing a comprehensive test suite (unit, integration, E2E), generating complete user documentation, API references, and providing practical usage examples and deployment scripts.",
        "details": "This task focuses on quality assurance, usability, and deployability:\n\n1.  **Comprehensive Test Suite (`tests/`)**: Expand the test suite to achieve >95% line coverage, >90% branch coverage, and 100% function coverage. Organize tests into `unit/`, `integration/`, and `e2e/` as per `test-strategy`. Use `pytest` (version 7.4.3+) for test execution and `pytest-cov` (version 4.1.0+) for coverage reporting. Implement `hypothesis` (version 6.94.0+) for property-based testing on mathematical functions (indicators, performance metrics). This ensures robust validation against a wide range of inputs and edge cases.\n2.  **Documentation (`docs/`)**: Write comprehensive user guides, installation instructions, API documentation, and troubleshooting FAQs. Use `Sphinx` (version 7.2.6+) for generating professional documentation from reStructuredText or Markdown files. Auto-generate API documentation from docstrings using `sphinx-autodoc`. Host documentation using `Read the Docs` or GitHub Pages.\n3.  **Examples (`examples/`, `notebooks/`)**: Create practical usage examples (e.g., Python scripts for specific tasks) and Jupyter notebooks (version 7.0.6+) for step-by-step walkthroughs of the analysis pipeline. Include example datasets and expected outputs. Ensure all examples are runnable and demonstrate key features.\n4.  **Deployment & Installation Scripts (`scripts/`)**: Provide clear installation instructions (`README.md`). Create deployment scripts if necessary (e.g., for packaging as a Docker image using `docker` 24.0.7+ or setting up environment with `Poetry`). Ensure project adheres to `PEP 517/PEP 621` for build backend definition in `pyproject.toml`.\n\n**Code Quality:** Enforce code style with `black` (version 23.12.1+) and ensure static type checking with `mypy` (version 1.7.1+) across the entire codebase. Integrate these into pre-commit hooks using `pre-commit` (version 3.5.0+).\n\n**Recommended Libraries & Versions:**\n*   `pytest`: 7.4.3+\n*   `pytest-cov`: 4.1.0+\n*   `hypothesis`: 6.94.0+\n*   `Sphinx`: 7.2.6+\n*   `sphinx-autodoc`: (part of Sphinx)\n*   `Jupyter Notebook`: 7.0.6+\n*   `docker`: 24.0.7+\n*   `black`: 23.12.1+\n*   `mypy`: 1.7.1+\n*   `pre-commit`: 3.5.0+",
        "testStrategy": "Execute the full test suite (`pytest --cov=src --cov-report=term-missing`). Verify coverage reports meet the specified requirements (>95% line, >90% branch, 100% function). Conduct a thorough documentation review for clarity, accuracy, and completeness. Run all example scripts and Jupyter notebooks to validate their correctness and reproducibility. Perform installation tests on clean environments (e.g., fresh OS, Docker container) to ensure smooth deployment. Critical scenarios: Identifying undocumented features, ensuring all error paths are tested, validating property-based tests catch subtle calculation errors, verifying deployment steps on target platforms.",
        "priority": "low",
        "dependencies": [
          9
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Structured Logging & Error Tracking",
            "description": "Integrate structured logging across both backend and frontend components, and set up a centralized error tracking and alerting system (e.g., Sentry) to capture, categorize, and notify on application errors, improving diagnosability and incident response.",
            "dependencies": [],
            "details": "1.  **Backend Structured Logging**: Configure Python's `logging` module to output logs in a structured format (e.g., JSON) including timestamps, log level, message, and relevant context (e.g., request ID, user ID) using `python-json-logger`. Define appropriate logging levels (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n2.  **Frontend Structured Logging**: Implement a client-side logging mechanism to capture significant events and errors, formatted for consistency with backend logs if possible.\n3.  **Error Tracking (Sentry)**: Integrate the Sentry SDK into both backend (e.g., Python Flask/Django) and frontend (e.g., React/Vue.js) applications. Configure it to automatically report unhandled exceptions and errors. Ensure PII is sanitized.\n4.  **Alerting Configuration**: Set up Sentry alerts for critical error rates, new error types, and specific error patterns. Define notification channels (e.g., Slack, email).\n5.  **Documentation**: Document the logging configurations, log formats, error tracking setup, and alerting rules.",
            "status": "pending",
            "testStrategy": "Trigger specific backend and frontend errors (e.g., division by zero, API call failure) in a staging environment to verify Sentry captures them correctly with relevant context. Check log files to ensure structured JSON output. Test alert notifications."
          },
          {
            "id": 2,
            "title": "Configure Performance Monitoring & User Analytics",
            "description": "Set up comprehensive performance monitoring for backend and frontend applications, including metrics collection for response times, resource utilization, and implement user analytics/event tracking to gain insights into application usage and user behavior.",
            "dependencies": [
              1
            ],
            "details": "1.  **Backend Performance Monitoring**: \n    *   Instrument key API endpoints and critical functions to collect metrics such as request latency, CPU usage, memory consumption, and error rates. \n    *   Integrate with a monitoring system (e.g., Prometheus/Grafana, Datadog) using appropriate client libraries (e.g., `prometheus_client` for Python). \n    *   Create Grafana dashboards (or similar) to visualize these metrics over time.\n2.  **Frontend Performance Monitoring**: \n    *   Measure crucial client-side performance metrics like page load times (FCP, LCP), Time To Interactive (TTI), and asset loading times. \n    *   Utilize Web Performance APIs or integrate dedicated frontend APM tools (if chosen).\n3.  **User Analytics/Event Tracking**: \n    *   Define key user interactions and events to track (e.g., login success, feature usage, report generation, data exports). \n    *   Integrate an analytics platform (e.g., Google Analytics 4, PostHog, Mixpanel) to collect and analyze these events. \n    *   Ensure data collection adheres to privacy regulations (e.g., GDPR, CCPA) through anonymization or consent mechanisms.\n4.  **Documentation**: Document all collected performance metrics, the structure of tracked user events, and dashboard locations/access procedures.",
            "status": "pending",
            "testStrategy": "Conduct load tests on backend endpoints and perform user journey simulations on the frontend to verify performance metrics are accurately captured and displayed in monitoring dashboards. Verify user analytics events are correctly registered in the chosen analytics platform upon specific user actions."
          },
          {
            "id": 3,
            "title": "Develop Health Check Endpoints & Operational Documentation",
            "description": "Implement robust health check endpoints for the application to report its status and develop comprehensive documentation for critical operational procedures including backup/recovery, security monitoring, and incident response.",
            "dependencies": [],
            "details": "1.  **Health Check Endpoints**: \n    *   Create a simple `/health` endpoint that returns a 200 OK when the application process is running (liveness probe). \n    *   Create a more comprehensive `/ready` endpoint that checks the status of essential dependencies (e.g., database connection, external services, critical configurations) and returns a 200 OK only if all dependencies are healthy (readiness probe). Include detailed status messages in the response body.\n2.  **Backup & Recovery Procedures**: \n    *   Document detailed, step-by-step procedures for backing up all critical data (database, configuration files, uploaded assets). \n    *   Outline the process for restoring the application and its data from backups, including required tools and verification steps. \n    *   Specify backup frequency, retention policies, and storage locations.\n3.  **Security Monitoring Procedures**: \n    *   Document procedures for reviewing security-related logs (e.g., authentication attempts, access controls, anomaly detection from monitoring systems).\n    *   Define incident response protocols for detected security events, including escalation paths and communication plans.\n4.  **Deployment Integration**: Ensure health check endpoints are properly configured in deployment environments (e.g., Kubernetes liveness and readiness probes) to facilitate automated restarts and traffic management.",
            "status": "pending",
            "testStrategy": "Manually test `/health` and `/ready` endpoints, simulating dependency failures (e.g., disconnect database) to ensure accurate status reporting. Conduct a tabletop exercise for documented backup/recovery and security incident response procedures with key stakeholders."
          }
        ]
      },
      {
        "id": 11,
        "title": "Fix Test Suite to Achieve 95% Coverage Using BTC.csv",
        "description": "Systematically align the existing test suite with the actual implementation behavior to resolve expectation mismatches. The core system is functional, but tests expected different behaviors (e.g., exceptions instead of warnings). The goal is to correct the tests, not the implementation, and, building on recent successes, increase test coverage to over 95% using the BTC.csv dataset. Significant progress has been made, doubling test coverage from 8.47% to 18.32% through a research-backed systematic approach.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "Initial investigation and fixes have revealed that the core logic in modules like `src/hmm_trading/data_processing`, `src/hmm_trading/feature_engineering`, and `src/hmm_trading/models` is correct and production-ready. The initial low test coverage was primarily due to a disconnect between the tests and the implementation. A research-backed systematic approach has been applied, yielding substantial progress.\n\nKey issues successfully addressed:\n1.  **Exception vs. Warning Mismatch**: Critical `ValueError` vs. warning mismatches have been resolved in data processing validation, specifically in `src/hmm_trading/data_processing/validation.py`. However, further analysis revealed that the implementation uses `logging.warning` rather than `warnings.warn`, meaning existing `pytest.warns(UserWarning)` assertions might need adjustment to use `pytest.caplog`.\n2.  **Function Signature Discrepancies**: HMM model training function calls have been corrected to match the actual implementation signatures in modules like `src/hmm_trading/models/model_training.py`.\n3.  **Integration Test Failures**: End-to-end pipeline tests using 1,005 rows of real BTC market data are now successfully completing, indicating that the HMM model training in `src/hmm_trading/models/model_training.py` converges as expected with adequate data.\n\n**Current Status:**\n-   23 tests are now passing (up from an initial 2-3).\n-   Code coverage has increased significantly from 8.47% to 18.32%.\n-   All critical data processing functionality is working, including validation logic.\n-   HMM model training is successfully completing with real market data.\n-   The complete integration pipeline is functional.\n\n**Remaining Work for 95% Coverage:**\n-   Systematically address the approximately 15 remaining test failures related to function signature discrepancies, particularly in areas like `src/hmm_trading/feature_engineering/indicators.py`.\n-   Re-evaluate and correct existing warning tests to properly handle `logging.warning` calls using `pytest.caplog`.\n-   Apply established research-backed patterns to fix tests in remaining modules.\n-   Add targeted unit and integration tests for currently uncovered code paths to close coverage gaps.\n-   Expand integration test scenarios to cover more comprehensive pipeline behaviors.",
        "testStrategy": "The strategy is to continue refactoring and expanding the test suite to match the implementation, building on the initial research-backed approach. This involves:\n1.  **Addressing Logger Warnings**: Systematically reviewing existing unit tests, particularly in `tests/unit/test_validation.py`, where `pytest.warns(UserWarning)` was used. These tests will be updated to use `pytest.caplog` to assert on `logging.warning` messages, accurately reflecting the implementation's warning mechanism.\n2.  **Completing Function Signature Alignment**: For the remaining ~15 failing unit tests, inspect corresponding source code (e.g., in `src/hmm_trading/feature_engineering/indicators.py`) and correct function calls in test files (e.g., `tests/unit/test_indicators.py`) to match current signatures.\n3.  **Expanding Test Coverage**: After aligning the existing tests, new unit and integration tests will be added. This will involve running `pytest --cov=src --cov-report=term-missing` to identify uncovered lines and branches. Priority will be given to critical logic in data processing, feature engineering, and model training modules. More E2E integration test scenarios will be added using the `BTC.csv` data to ensure all components work together as expected.\n4.  **Continuous Verification**: Continuously run the coverage report and the full test suite to track progress towards the >95% goal. A final successful run of the full test suite with the target coverage will mark this task as complete.",
        "subtasks": [
          {
            "id": 1,
            "title": "Setup Test Fixtures and Data Infrastructure",
            "description": "Integrate the `BTC.csv` dataset as a shared pytest fixture and establish the necessary infrastructure for loading and using this data across unit and integration tests.",
            "dependencies": [],
            "details": "This subtask involved creating a fixture in `tests/conftest.py` to load the `BTC.csv` file from `tests/fixtures/` into a pandas DataFrame. This ensures a consistent and reusable data source for all subsequent tests, improving test reliability and reducing boilerplate code.",
            "status": "completed",
            "testStrategy": "Verification was done by creating a simple test case that uses the fixture to load the data and asserts that the resulting DataFrame is not empty and has the expected column structure. This test now serves as a baseline for data-dependent tests."
          },
          {
            "id": 2,
            "title": "Initial Test Fixes and Root Cause Analysis",
            "description": "Perform an initial pass on the test suite to fix low-hanging issues, which led to the identification of the systemic mismatch between test expectations and implementation behavior. This effort increased code coverage from 8.47% to 15.73%.",
            "dependencies": [],
            "details": "Fixed 3 tests in `tests/unit/test_validation.py` and confirmed basic functionality in feature engineering tests. The investigation confirmed the core implementation is sound, but tests were written with incorrect assumptions (expecting exceptions instead of warnings, using old function signatures).",
            "status": "completed",
            "testStrategy": "Run the entire test suite using `pytest` and analyze the initial set of failures. Correcting the most straightforward failures confirmed the root cause and provided a clear path forward for the remaining tests."
          },
          {
            "id": 3,
            "title": "Align Unit Tests with Implementation Behavior",
            "description": "Systematically review and refactor the remaining ~50 failing unit tests to align with the actual implementation logic, primarily focusing on exception-vs-warning mismatches and incorrect function signatures. This included fixing critical ValueError vs warning mismatches in data processing validation and correcting some function signature discrepancies.",
            "dependencies": [
              2
            ],
            "details": "This task involved modifying tests across multiple files, including `tests/unit/test_validation.py` and `tests/unit/test_indicators.py`. Significant progress was made in replacing `with pytest.raises(ValueError):` blocks where appropriate and updating function call arguments to match current signatures. This contributed to increasing the number of passing tests and overall coverage.",
            "status": "completed",
            "testStrategy": "For each failing test, inspect the corresponding source code to understand the intended behavior. Apply the appropriate fix in the test file and re-run the specific test to confirm it passes. This process was continued until a substantial portion of existing unit tests passed, confirming alignment."
          },
          {
            "id": 4,
            "title": "Resolve HMM Training Integration Test Failures",
            "description": "Fix the integration tests in `tests/integration/test_e2e_pipeline.py` that fail due to HMM model convergence issues on small data slices. This now includes successfully completing end-to-end pipeline tests using 1,005 rows of real BTC market data.",
            "dependencies": [
              2
            ],
            "details": "The `hmmlearn` library, used in `src/hmm_trading/models/model_training.py`, can throw errors or fail to converge when the input time series is too short. This task involved adjusting the integration test in `tests/integration/test_e2e_pipeline.py` to use a sufficiently large slice of `BTC.csv` (1,005 rows) that allows for model convergence. The HMM model now trains successfully, and the complete integration pipeline is functional.",
            "status": "completed",
            "testStrategy": "Run the integration test suite and isolate the HMM training failure. The solution involved adjusting the test data size in `tests/integration/test_e2e_pipeline.py` to ensure convergence. The test now passes by successfully training the HMM model on a revised dataset and validating the end-to-end pipeline functionality."
          },
          {
            "id": 5,
            "title": "Expand Test Suite to Achieve 95% Coverage",
            "description": "With the existing test suite fixed and aligned, write new unit and integration tests to close the remaining coverage gaps and reach the 95% target.",
            "dependencies": [
              3,
              4
            ],
            "details": "Run `pytest --cov=src --cov-report=term-missing` to identify uncovered lines and branches in the codebase. Prioritize writing tests for critical, uncovered logic in the data processing, feature engineering, and model training modules. Add more E2E integration test scenarios using the `BTC.csv` data to ensure all components work together as expected.",
            "status": "done",
            "testStrategy": "Continuously run the coverage report after adding new tests to track progress towards the 95% goal. Ensure all new tests follow the established pattern of aligning with the actual, correct implementation behavior. A final successful run of the full test suite with the target coverage will mark this task as complete."
          },
          {
            "id": 6,
            "title": "Re-evaluate and Correct Warning-Related Tests for Logger Integration",
            "description": "The current implementation uses `logging.warning` instead of `warnings.warn`. Review existing unit tests, particularly in `tests/unit/test_validation.py`, where `pytest.warns(UserWarning)` was used. Update these tests to correctly capture and assert on log messages using `pytest.caplog` or similar `pytest` logging fixtures to align with the actual logging mechanism.",
            "dependencies": [
              3
            ],
            "details": "Investigate `src/hmm_trading/data_processing/validation.py` to confirm usage of `logging.warning`. Identify all test cases in `tests/unit/test_validation.py` (and potentially other modules) that currently use `pytest.warns`. Refactor these tests to utilize the `caplog` fixture to assert on the content and level of `logging` messages, ensuring accurate validation of warning conditions.",
            "status": "done",
            "testStrategy": "Run the specific warning-related unit tests before and after refactoring. Verify that tests fail if the expected log warning is not present or if the log content/level is incorrect, and pass when the implementation behaves as expected and logs correctly. Ensure no unintended side effects on other tests."
          },
          {
            "id": 7,
            "title": "Systematic Fixes for Remaining Function Signature Discrepancies (~15 tests)",
            "description": "Address the remaining approximately 15 unit test failures primarily caused by function signature mismatches. This involves systematically inspecting function calls in test files (e.g., `tests/unit/test_indicators.py`) and ensuring they align with the latest function definitions in source modules (e.g., `src/hmm_trading/feature_engineering/indicators.py`).",
            "dependencies": [
              3
            ],
            "details": "Identify the ~15 failing unit tests where argument discrepancies are still present. For each test, cross-reference the function call parameters with the actual function signature in the corresponding source file. Update test calls to use correct parameter names and order (e.g., `config` instead of `indicator_config`). Focus areas include `tests/unit/test_indicators.py` and other modules identified by `pytest` failures.",
            "status": "done",
            "testStrategy": "For each identified failing test, apply the function signature correction and re-run only that specific test to confirm it passes. Periodically run the full unit test suite to ensure no regressions and to track the reduction in failing tests. The goal is to bring all ~15 remaining tests to a passing state."
          }
        ]
      },
      {
        "id": 12,
        "title": "Fix Critical CLI Entry Point Configuration",
        "description": "Correct the misconfigured CLI entry points in pyproject.toml by relocating the cli_simple.py script to the src/ directory, ensuring the 'hmm-analyze' and 'hmm-futures' commands function correctly after package installation.",
        "details": "The current project configuration in `pyproject.toml` under `[project.scripts]` defines two entry points, 'hmm-analyze' and 'hmm-futures', both pointing to `src.cli_simple:main`. However, the file `cli_simple.py` currently resides in the project root, not within the `src/` directory, leading to a `ModuleNotFoundError` when the commands are executed from an installed package. The correct approach, aligning with the project's `src`-layout structure, is to move the script into the `src` package. No changes are needed in `pyproject.toml` as it already reflects the target state.\n\nImplementation Steps:\n1. Move the CLI script to its correct location: `mv cli_simple.py src/cli_simple.py`.\n2. Verify that the imports within the newly moved `src/cli_simple.py` still function correctly. Given the `src` layout, absolute imports like `from hmm_trading.data_processing import ...` should resolve without modification.\n3. Re-install the package in editable mode to update the entry point links: `pip install -e .`.",
        "testStrategy": "The primary goal is to verify that the command-line tools work correctly after installation.\n1. Perform a clean, editable installation of the package: `pip uninstall hmm-trading -y && pip install -e .`.\n2. Execute the `hmm-analyze` command's help flag to confirm the entry point is resolved: `hmm-analyze --help`. The output should be the Click-generated help message.\n3. Execute the `hmm-futures` command's help flag: `hmm-futures --help`. This should also display the correct help message.\n4. Run a full analysis command to ensure the script executes end-to-end: `hmm-analyze data/BTC.csv --states 3`. Verify that it runs without a `ModuleNotFoundError` and produces the expected analysis output.",
        "status": "done",
        "dependencies": [
          9
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Move cli_simple.py Script to src Directory",
            "description": "Relocate the `cli_simple.py` script from the project root into the `src/` directory. This is the first step to align the file structure with the entry point configuration defined in `pyproject.toml`.",
            "dependencies": [],
            "details": "Execute the command `mv cli_simple.py src/cli_simple.py`. This action is necessary because the `[project.scripts]` section in `pyproject.toml` points to `src.cli_simple:main`, which requires the script to be inside the `src` package.",
            "status": "done",
            "testStrategy": "Verify using `ls src/cli_simple.py` that the file has been successfully moved into the `src` directory and no longer exists in the project root."
          },
          {
            "id": 2,
            "title": "Ensure src Directory is a Discoverable Python Package",
            "description": "To make the `cli_simple` module importable as `src.cli_simple`, the `src` directory must be a valid Python package. This requires creating an `__init__.py` file within it if one does not already exist.",
            "dependencies": [
              1
            ],
            "details": "Check for the existence of `src/__init__.py`. If the file is missing, create it as an empty file using `touch src/__init__.py`. This makes the `src` directory a package, allowing the setuptools entry point mechanism to discover and use `src.cli_simple`.",
            "status": "done",
            "testStrategy": "Confirm that the file `src/__init__.py` exists after this step. This can be checked with a simple file listing command."
          },
          {
            "id": 3,
            "title": "Re-install Package and Verify CLI Entry Point Functionality",
            "description": "After relocating the script and ensuring the package structure is correct, re-install the package in editable mode to update the links to the CLI entry points. Finally, verify that the `hmm-analyze` and `hmm-futures` commands are now functional.",
            "dependencies": [
              1,
              2
            ],
            "details": "First, uninstall any existing version: `pip uninstall hmm-trading -y`. Then, run `pip install -e .` to re-install the package with the updated structure. After installation, execute `hmm-analyze --help` and `hmm-futures --help` to confirm they run without a `ModuleNotFoundError`.",
            "status": "done",
            "testStrategy": "Perform a clean, editable installation of the package: `pip uninstall hmm-trading -y && pip install -e .`. Then, execute `hmm-analyze --help` and `hmm-futures --help`. A successful test is one where both commands display their help messages without any import errors."
          }
        ]
      },
      {
        "id": 13,
        "title": "Resolve Test Import Errors for StrategyEngine",
        "description": "Fix Python import errors in the test suite, specifically for the StrategyEngine module, by correctly configuring the test environment to recognize the src-layout project structure.",
        "details": "The test suite is currently failing because tests cannot resolve imports from the `src/` directory, a common issue in projects with a `src`-layout. This is evident with modules like `src/backtesting/strategy_engine.py`. The solution is to configure `pytest` to recognize the `src` directory as a valid path for module discovery.\n\n1.  **Configure Pytest Paths**: Modify the `pyproject.toml` file to include pytest configuration that explicitly adds the `src` directory to the `PYTHONPATH` during test runs. This ensures that imports like `from hmm_trading.backtesting.strategy_engine import StrategyEngine` are resolved correctly.\n\n    Add or update the following section in `pyproject.toml`:\n    ```toml\n    [tool.pytest.ini_options]\n    pythonpath = [\"src\"]\n    addopts = \"--import-mode=importlib\"\n    ```\n\n2.  **Verify Import Statements**: Review test files, particularly any related to `strategy_engine.py`, to ensure they use absolute imports relative to the `src` directory (e.g., `from hmm_trading.backtesting.strategy_engine import ...`) rather than relative imports that may fail.\n\n3.  **Ensure Package Structure**: Confirm that all necessary `__init__.py` files are present within the `src/hmm_trading` subdirectories to ensure they are treated as Python packages, although this is less critical with modern Python versions.",
        "testStrategy": "The primary goal is to confirm that the test suite can be collected and run without any `ImportError` or `ModuleNotFoundError` exceptions.\n\n1.  **Test Collection**: Run `pytest --collect-only` from the project root. This command should execute successfully and list all discovered tests without any import-related errors.\n2.  **Test Execution**: Run the full test suite using the command `pytest`. The suite should start running tests. While other test failures (e.g., `AssertionError`) are not in the scope of this task, the run must not be halted by any import errors.\n3.  **Specific Module Test**: Target a previously failing test file directly to confirm the fix: `pytest tests/backtesting/test_strategy_engine.py`. This test should now run without import issues.",
        "status": "done",
        "dependencies": [
          11,
          12
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Pytest to Recognize 'src' Directory",
            "description": "Modify the `pyproject.toml` file to add a pytest configuration that includes the `src` directory in the PYTHONPATH. This is the first step to resolving module import errors in a `src`-layout project.",
            "dependencies": [],
            "details": "Add the `[tool.pytest.ini_options]` section to `pyproject.toml` with the following keys: `pythonpath = [\"src\"]` and `addopts = \"--import-mode=importlib\"`. This configuration instructs pytest to treat the `src` directory as a root for package discovery during test execution.",
            "status": "done",
            "testStrategy": "After this change, running `pytest --collect-only` might still show import errors because the import statements in test files are not yet corrected. However, this configuration is a prerequisite for the next step."
          },
          {
            "id": 2,
            "title": "Update Test Module Imports to be Relative to 'src'",
            "description": "Refactor the import statements within the test suite, specifically in files testing the StrategyEngine, to use absolute imports from the top-level package (`hmm_trading`) instead of from `src`.",
            "dependencies": [
              1
            ],
            "details": "Search all files in the `tests/` directory for imports starting with `from src.hmm_trading...`. Change them to `from hmm_trading...`. For example, `from src.hmm_trading.backtesting.strategy_engine import StrategyEngine` must be updated to `from hmm_trading.backtesting.strategy_engine import StrategyEngine`.",
            "status": "done",
            "testStrategy": "Run `pytest --collect-only` from the project root. The command should now complete successfully and discover all tests without raising any `ModuleNotFoundError` or `ImportError` exceptions."
          },
          {
            "id": 3,
            "title": "Execute Full Test Suite and Verify Package Integrity",
            "description": "Run the entire test suite to confirm that all import errors are resolved and the tests execute successfully. Also, perform a final verification of the package structure by ensuring `__init__.py` files are present.",
            "dependencies": [
              2
            ],
            "details": "From the project root, execute the `pytest` command. All tests should run without import-related failures. As a final check, confirm that `src/hmm_trading/__init__.py` and `src/hmm_trading/backtesting/__init__.py` exist to ensure proper package recognition.",
            "status": "done",
            "testStrategy": "The primary success criterion is a full, passing test run invoked by the `pytest` command. The terminal output should show that all collected tests have passed without any errors, particularly import errors."
          }
        ]
      },
      {
        "id": 14,
        "title": "Enhance Code Quality by Resolving Linter and Type-Checking Errors",
        "description": "Resolve over 80 ruff linting violations and all mypy type-checking errors across the codebase, and modernize configurations for ruff and Pydantic V2.",
        "details": "This task involves a comprehensive code quality overhaul. The primary goal is to eliminate all reported issues from `ruff` and `mypy`, bringing the codebase to a higher standard of maintainability and correctness.\n\n1.  **Update `pyproject.toml` Configuration**:\n    *   Migrate the `[tool.ruff]` configuration to the modern format. The existing settings should be moved into new tables like `[tool.ruff.lint]` and `[tool.ruff.format]`. \n    *   Add `types-PyYAML` to the development dependencies (e.g., under `[project.optional-dependencies.dev]`) to provide type stubs for the PyYAML library, which will resolve a class of `mypy` errors.\n\n2.  **Migrate Pydantic V1 Validators to V2**:\n    *   Search the codebase for Pydantic models using the deprecated `@validator` decorator.\n    *   Update them to use the Pydantic V2 `@field_validator` decorator. This also involves updating the validator function signature.\n    *   Example Migration:\n        *   **V1 (Before):**\n          ```python\n          from pydantic import BaseModel, validator\n          class MyModel(BaseModel):\n              value: int\n              @validator('value')\n              def check_value(cls, v):\n                  if v < 0: raise ValueError('value must be non-negative')\n                  return v\n          ```\n        *   **V2 (After):**\n          ```python\n          from pydantic import BaseModel, field_validator\n          class MyModel(BaseModel):\n              value: int\n              @field_validator('value')\n              @classmethod\n              def check_value(cls, v: int) -> int:\n                  if v < 0: raise ValueError('value must be non-negative')\n                  return v\n          ```\n\n3.  **Resolve Ruff Linting Errors (81+)**:\n    *   Run `ruff check . --add-select I --fix` to automatically fix as many issues as possible, including import order problems.\n    *   Manually address the remaining errors:\n        *   **Unused Variables (F841):** Remove the variable or prefix it with an underscore (e.g., `_unused_var`) if it's intentionally unused.\n        *   **Bare Excepts (E722):** Replace generic `except:` clauses with `except Exception as e:` to avoid catching `SystemExit` and `KeyboardInterrupt`.\n        *   **Other Import Issues:** Manually resolve any remaining import-related warnings that the auto-fix could not handle.\n\n4.  **Fix Mypy Type-Checking Failures**:\n    *   Run `mypy src tests` to get a list of all type errors.\n    *   Systematically add missing type hints to function arguments, return values, and variables until `mypy` passes without errors.",
        "testStrategy": "The success of this task will be validated by static analysis tools and the existing test suite to ensure no regressions were introduced.\n\n1.  **Linter Verification**: Run `ruff check .` from the project root. The command must complete successfully with zero reported errors.\n\n2.  **Formatter Verification**: Run `ruff format . --check`. The command must report that all files are formatted correctly.\n\n3.  **Type-Checking Verification**: Run `mypy src tests`. The command must complete with the message 'Success: no issues found'.\n\n4.  **Regression Testing**: Execute the full test suite using `pytest`. All tests must pass to confirm that the code quality improvements have not broken existing functionality.\n\n5.  **CLI Sanity Check**: After the fixes are applied, run a basic command like `hmm-analyze --help` to ensure the application's entry point is still functioning correctly.",
        "status": "in-progress",
        "dependencies": [
          11,
          12,
          13
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update pyproject.toml with Modern Ruff Config and Type Stubs",
            "description": "Modernize the `pyproject.toml` file by migrating the Ruff configuration to the new format with `[tool.ruff.lint]` and `[tool.ruff.format]` tables. Also, add `types-PyYAML` to the development dependencies to resolve mypy errors related to the PyYAML library.",
            "dependencies": [],
            "details": "Locate the `pyproject.toml` file. Under the `[tool.ruff]` section, move settings like `select`, `ignore`, etc., into a new `[tool.ruff.lint]` table. Move formatter settings like `line-length` into `[tool.ruff.format]`. Find the `[project.optional-dependencies.dev]` section and add 'types-PyYAML' to the list.",
            "status": "done",
            "testStrategy": "Verify the changes by running `ruff check .` and `ruff format . --check` to ensure the new configuration is correctly parsed and applied. Also, install dev dependencies using `pip install -e '.[dev]'` and confirm `types-PyYAML` is installed."
          },
          {
            "id": 2,
            "title": "Migrate Pydantic V1 Validators to V2 @field_validator",
            "description": "Search the codebase for all instances of the deprecated Pydantic V1 `@validator` decorator and update them to use the modern Pydantic V2 `@field_validator` decorator, including adjusting the function signatures as required.",
            "dependencies": [
              1
            ],
            "details": "Use a codebase search tool (like grep or an IDE feature) to find all occurrences of `@validator`. For each instance, replace it with `@field_validator` and add the `@classmethod` decorator. Update the function signature to include type hints, e.g., `def check_value(cls, v: int) -> int:`. Ensure all models using these validators are correctly imported from `pydantic`.",
            "status": "in-progress",
            "testStrategy": "Run the project's existing test suite to ensure that the Pydantic model validation logic still works as expected after the migration. Manually inspect the models to confirm the new decorators are correctly applied."
          },
          {
            "id": 3,
            "title": "Fix All Ruff Linting Violations (81+)",
            "description": "Execute a comprehensive linting pass to resolve all reported ruff errors. This includes running an auto-fix command and then manually addressing any remaining issues, such as unused variables and bare except clauses.",
            "dependencies": [
              1
            ],
            "details": "First, run `ruff check . --add-select I --fix` to automatically correct import order issues and other simple violations. Afterward, run `ruff check .` again. Manually fix remaining errors: for `F841` (unused variable), delete the variable or rename it to `_variable`. For `E722` (bare except), change `except:` to `except Exception:`. Continue until the command reports zero errors.",
            "status": "pending",
            "testStrategy": "The primary verification is to run `ruff check .` from the project root. The command must exit with a status code of 0, indicating no linting errors remain."
          },
          {
            "id": 4,
            "title": "Resolve All Mypy Type-Checking Errors",
            "description": "Achieve a clean mypy run by systematically addressing all reported type-checking errors. This involves adding missing type hints to function signatures, variables, and class attributes across the `src` and `tests` directories.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Run the command `mypy src tests`. For each reported error, navigate to the specified file and line. Add the necessary type annotations to function arguments (`def my_func(arg: str)`), return values (`-> bool:`), and variables. Repeat the process until the `mypy` command runs successfully with no errors found.",
            "status": "pending",
            "testStrategy": "Verification is complete when running `mypy src tests` from the project root produces the output 'Success: no issues found in X source files'."
          }
        ]
      },
      {
        "id": 15,
        "title": "Enhance Makefile with Development Workflow Targets",
        "description": "Add new targets to the Makefile for auto-fixing lint issues, live documentation previews, continuous testing, enhanced cache cleaning, and code quality reporting to streamline the development process.",
        "details": "This task involves expanding the existing `Makefile` to include several new targets that automate common development tasks, leveraging tools like `ruff`, `mkdocs`, and `pytest`. This will reduce manual effort and improve developer productivity.\n\n1.  **`lint-fix` Target**: Create a new target that uses `ruff check . --fix` to automatically correct fixable linting violations. This should be separate from the existing `lint` target, which should only report issues.\n\n2.  **`docs-live` Target**: Add a target to serve the project documentation locally with live-reloading. This will use the command `mkdocs serve`, allowing developers to preview documentation changes in real-time as they edit Markdown files in the `docs/` directory.\n\n3.  **`test-watch` Target**: Implement a continuous testing target. Add `pytest-watch` to the development dependencies in `pyproject.toml`. The make target should execute `ptw -- -n auto --cov=src`, which will monitor `src/**/*.py` and `tests/**/*.py` for changes and automatically re-run the `pytest` suite.\n\n4.  **`clean-py` Target**: Create a comprehensive cleaning target to remove all Python-related cache and build artifacts. This target should use `find` commands to recursively delete `__pycache__` directories, `.pyc` files, and temporary directories like `.pytest_cache`, `.mypy_cache`, `build/`, `dist/`, and `*.egg-info/`.\n\n5.  **`coverage-report` Target**: Create a target that generates an HTML code coverage report and opens it in the browser. The command should be `pytest --cov=src --cov-report=html && open htmlcov/index.html` (or equivalent for Linux/Windows) to provide immediate access to the coverage results.\n\nAll new targets must be declared with `.PHONY` to ensure correct behavior.",
        "testStrategy": "Each new Makefile target must be verified individually from the project root directory.\n\n1.  **Verify `lint-fix`**: Intentionally introduce a simple, auto-fixable linting error (e.g., an unused import) into a Python file. Run `make lint` to confirm the error is reported. Then, run `make lint-fix` and verify that the code has been automatically corrected and `make lint` now passes without errors.\n\n2.  **Verify `docs-live`**: Run `make docs-live`. Access the documentation at `http://127.0.0.1:8000` in a web browser. Modify a file in the `docs/` directory and confirm that the browser automatically reloads to show the changes.\n\n3.  **Verify `test-watch`**: Run `make test-watch`. Confirm the test suite runs successfully once. Modify a test file in `tests/` and a source file in `src/` and verify that the test suite is automatically triggered to run again in both cases.\n\n4.  **Verify `clean-py`**: First, run `make test` to ensure cache directories like `.pytest_cache` and `__pycache__` are generated. Then, run `make clean-py` and use `ls -a` or `find` to confirm that all specified cache directories and files have been removed.\n\n5.  **Verify `coverage-report`**: Run `make coverage-report`. Verify that the command completes successfully, an `htmlcov/` directory is created, and the `index.html` report is automatically opened in the default web browser.",
        "status": "done",
        "dependencies": [
          10,
          14
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Add `lint-fix` Target to Makefile for Automatic Linting Fixes",
            "description": "Create a new `.PHONY` target named `lint-fix` in the Makefile to automatically apply fixable linting corrections using `ruff`, separating fixing from the existing reporting-only `lint` target.",
            "dependencies": [],
            "details": "In the `Makefile`, add the `.PHONY: lint-fix` declaration. Implement the `lint-fix` target to execute the command `ruff check . --fix`. This will provide a convenient way for developers to auto-correct code style and quality issues directly from the command line.",
            "status": "done",
            "testStrategy": "Intentionally introduce a simple, auto-fixable linting error (e.g., an unused import) into a Python file. Run `make lint` to confirm the error is reported. Then, run `make lint-fix` and verify the command executes successfully and the error is automatically corrected in the file."
          },
          {
            "id": 2,
            "title": "Implement `docs-live` Makefile Target for Live Documentation Preview",
            "description": "Add a `.PHONY` target named `docs-live` to the Makefile that serves the project documentation locally with live-reloading capabilities using `mkdocs`.",
            "dependencies": [],
            "details": "Add the target `.PHONY: docs-live` to the `Makefile`. The implementation for this target should run the command `mkdocs serve`. This will start a local development server, allowing developers to preview changes to documentation files in the `docs/` directory in real-time in their browser.",
            "status": "done",
            "testStrategy": "Run `make docs-live` from the project root. Verify that a local server starts (e.g., at `http://127.0.0.1:8000`). Access the URL in a browser to view the documentation. Make a small change to a `.md` file in the `docs/` directory and confirm the browser page automatically reloads with the updated content."
          },
          {
            "id": 3,
            "title": "Integrate Continuous Testing with `test-watch` Makefile Target",
            "description": "Add the `pytest-watch` dependency to the project and create a `test-watch` target in the Makefile to automatically re-run tests whenever source or test files are changed.",
            "dependencies": [],
            "details": "First, add `pytest-watch` to the development dependencies in the `[tool.poetry.group.dev.dependencies]` section of `pyproject.toml`. Then, add a `.PHONY: test-watch` target to the `Makefile` that executes `ptw -- -n auto --cov=src`. This command will monitor `src/**/*.py` and `tests/**/*.py` for changes and trigger the test suite upon detection.",
            "status": "done",
            "testStrategy": "After adding the dependency and running the project's install command, execute `make test-watch`. Modify a file in `src/` or `tests/`. Verify that the `pytest` suite automatically re-runs in the terminal, showing test results without manual intervention."
          },
          {
            "id": 4,
            "title": "Create Comprehensive `clean-py` Target for Python Artifacts",
            "description": "Implement a new `.PHONY` target `clean-py` in the Makefile to thoroughly remove Python-related cache files and build artifacts from the project directory.",
            "dependencies": [],
            "details": "In the `Makefile`, define a `.PHONY: clean-py` target. Use `find` commands to recursively locate and delete `__pycache__` directories, `.pyc` files, and other temporary directories such as `.pytest_cache`, `.mypy_cache`, `build/`, `dist/`, and `*.egg-info/`. This ensures a clean project state.",
            "status": "done",
            "testStrategy": "First, run tests (`make test`) and other commands to generate artifacts like `__pycache__` and `.pytest_cache`. Verify these files and directories exist. Then, run `make clean-py`. Verify that all the specified artifacts have been successfully deleted from the project tree by running a `find` or `ls` command."
          }
        ]
      },
      {
        "id": 16,
        "title": "Standardize Project-Wide Imports and Module Structure",
        "description": "Refactor the entire codebase to use consistent, absolute import paths relative to the 'src' directory and reorganize key modules to improve logical cohesion and maintainability.",
        "details": "Following the major code quality cleanup, this task will enforce a single, consistent import strategy across the project. Currently, the codebase may contain a mix of absolute and relative imports. Standardizing on absolute imports improves code readability and maintainability, especially in a `src`-layout project.\n\n1.  **Enforce Absolute Imports:** Systematically refactor all Python files in both `src/hmm_trading/` and `tests/` to use absolute imports. For example, an import in a test file should change from a relative path to `from hmm_trading.data_processing import file_utils`.\n\n2.  **Update Ruff Configuration:** Modify the `[tool.ruff.lint]` section in `pyproject.toml` to enforce this new convention. Add the `I002` (banned-relative-import) rule to the list of checked rules to prevent future relative imports.\n\n3.  **Module Cohesion Review:** Analyze the modules within `src/hmm_trading`, particularly large files like `utils.py` or `validation.py`. If these modules contain loosely related functions, refactor them by splitting them into smaller, more focused modules (e.g., `config_utils.py`, `data_validators.py`).\n\n4.  **Standardize `__init__.py`:** Ensure `src/hmm_trading/__init__.py` and other package `__init__.py` files are used consistently, either for defining a public API for the package or for namespace purposes, and document the chosen approach.",
        "testStrategy": "Verification will focus on ensuring the refactoring did not introduce regressions and that the new standards are correctly applied.\n\n1.  **Static Analysis Pass:** Run `make lint` from the project root. The command must complete with zero errors, specifically confirming that the newly enabled `I002` rule in ruff reports no violations.\n\n2.  **Full Test Suite Execution:** Execute the complete test suite by running `pytest` or `make test`. All existing tests must pass, proving that the changes to import paths and module structure have not broken any functionality.\n\n3.  **CLI Functionality Check:** Verify that the command-line entry points remain functional after the refactoring. Run `hmm-analyze --help` and `hmm-futures --help` to ensure they execute without import errors.\n\n4.  **Manual Code Review:** Conduct a spot-check of several files in both `src/` and `tests/` to manually confirm that relative imports have been eliminated and the new module structure is logical.",
        "status": "pending",
        "dependencies": [
          12,
          13,
          14
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor All Imports to Use Absolute Paths",
            "description": "Systematically review all Python files in `src/hmm_trading/` and `tests/` and convert any relative imports (e.g., `from ..utils import config`) to absolute imports (e.g., `from hmm_trading.utils import config`).",
            "dependencies": [],
            "details": "Use an IDE's refactoring tools or a script to find and replace all relative imports starting with '.' with absolute paths relative to the `src` directory. For example, an import `from .performance_metrics import calculate_metrics` in `strategy_engine.py` should become `from hmm_trading.backtesting.performance_metrics import calculate_metrics`.",
            "status": "pending",
            "testStrategy": "Run the full test suite using `pytest`. All tests must pass, specifically checking for any `ImportError` or `ModuleNotFoundError` exceptions that would indicate a failure in the refactoring process."
          },
          {
            "id": 2,
            "title": "Update Ruff Configuration to Ban Relative Imports",
            "description": "Modify the `pyproject.toml` file to add the Ruff linting rule `I002` (banned-relative-import) to prevent future relative imports from being introduced into the codebase.",
            "dependencies": [
              1
            ],
            "details": "In `pyproject.toml`, locate the `[tool.ruff.lint]` section. Add `\"I002\"` to the `select` or `extend-select` list. After this change, run `ruff check .` to confirm that the new rule is active and that no violations remain in the codebase.",
            "status": "pending",
            "testStrategy": "Run the project's linting command (e.g., `make lint` or `ruff check .`). The command must complete with zero errors, confirming that the configuration correctly enforces the no-relative-imports rule."
          },
          {
            "id": 3,
            "title": "Refactor `feature_engineering` Module for Improved Cohesion",
            "description": "Analyze the `src/hmm_trading/data_processing/feature_engineering.py` module and split it into smaller, more focused modules based on function. For example, separate technical indicator calculations from data validation or cleaning helpers.",
            "dependencies": [
              1
            ],
            "details": "Create a new module `src/hmm_trading/data_processing/validation.py`. Move functions related to data validation and cleaning from `feature_engineering.py` into this new file. Update all call sites across the codebase to use the new absolute import path for these moved functions.",
            "status": "pending",
            "testStrategy": "Execute the full test suite, paying close attention to tests for data processing and model training. Ensure that data pipelines execute correctly and that features are calculated as expected. If necessary, create a new test file `tests/test_validation.py` for the new module."
          },
          {
            "id": 4,
            "title": "Standardize Public API Exposure in `__init__.py` Files",
            "description": "Review all `__init__.py` files within `src/hmm_trading` sub-packages (`data_processing`, `models`, `utils`, etc.) and establish a consistent pattern for exposing a public API for each package.",
            "dependencies": [
              1,
              3
            ],
            "details": "Adopt a single convention for defining the public API in `__init__.py` files. Either use `from .module import ClassOrFunction` to promote key components to the package level or define `__all__ = [\"ClassOrFunction\"]`. Apply this convention consistently to all `__init__.py` files in `src/hmm_trading/` and its subdirectories. Add a module-level docstring to each `__init__.py` explaining the package's role.",
            "status": "pending",
            "testStrategy": "After refactoring, perform a static analysis pass with `mypy` to catch any import-related type errors. Run the full test suite to confirm that module imports at the package level (e.g., `from hmm_trading.data_processing import process_csv`) work correctly and have not introduced regressions."
          }
        ]
      },
      {
        "id": 17,
        "title": "Fix and Harden Testing Infrastructure",
        "description": "Correct pytest configuration by registering all test markers, fixing integration test setup, ensuring test coverage reporting functions correctly, and adding missing test dependencies to support the complete test suite.",
        "details": "This task addresses several configuration issues that are currently hampering the project's testing capabilities. A stable test environment is critical before further refactoring or feature development.\n\n1.  **Register Pytest Markers**: The test suite uses custom markers (e.g., `@pytest.mark.integration`) which cause warnings or errors if not registered. Add all custom markers to the `pyproject.toml` file under the `[tool.pytest.ini_options]` section to ensure `pytest --strict-markers` passes.\n    Example for `pyproject.toml`:\n    ```toml\n    [tool.pytest.ini_options]\n    markers = [\n        \"slow: marks tests as slow (deselect with -m 'not slow')\",\n        \"integration: marks integration tests\",\n        \"e2e: marks end-to-end tests\"\n    ]\n    ```\n\n2.  **Add Missing Test Dependencies**: The full test suite requires packages that are not yet listed as development dependencies. Identify and add these packages (e.g., `pytest-mock`, `freezegun`, `test-data-loader`) to the `[project.optional-dependencies.test]` group in `pyproject.toml`.\n\n3.  **Verify Coverage Configuration**: The coverage reporting defined in Task #10 needs to be validated. Ensure the `pytest-cov` configuration, likely managed via `[tool.coverage.run]` in `pyproject.toml`, correctly targets the `src/` directory for analysis and excludes the `tests/` directory from the report.\n\n4.  **Fix Integration Test Configuration**: The integration tests may have unique setup requirements (e.g., file path assumptions, specific fixtures) that are failing due to recent project structure changes. Investigate and fix the `tests/integration/conftest.py` or individual test files to align with the `src`-layout and ensure they can run independently and as part of the full suite.",
        "testStrategy": "Verification will focus on confirming that the entire testing toolchain operates without configuration errors.\n\n1.  **Marker Verification**: From the project root, run `pytest --strict-markers`. The command must complete without any warnings or errors related to unregistered markers.\n\n2.  **Dependency Verification**: In a clean virtual environment, run `pip install -e .[test]`. Then execute the full test suite via `pytest`. The suite should run without any `ModuleNotFoundError` or `ImportError` related to test-specific packages.\n\n3.  **Coverage Report Verification**: Run `pytest --cov=src/hmm_trading --cov-report=term-missing`. The command must execute successfully and produce a coverage report. Verify that the report only includes modules from within the `src/hmm_trading` directory.\n\n4.  **Integration Test Execution**: Run the integration test suite in isolation using `pytest tests/integration/`. All tests within this subdirectory must pass without errors.",
        "status": "pending",
        "dependencies": [
          10,
          13,
          14
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-10-19T08:32:45.168Z",
      "updated": "2025-10-27T11:39:51.671Z",
      "description": "Tasks for master context"
    }
  }
}
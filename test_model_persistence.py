#!/usr/bin/env python3
"""
Test script for HMM Model Persistence (Task 5.2).

This script tests the complete model persistence functionality including:
- Save and load operations with pickle serialization
- Integrity validation with hash checking
- Configuration reconstruction
- Model functionality validation
- File management utilities
"""

import os
import sys
import tempfile
from pathlib import Path

import numpy as np

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def create_test_model():
    """Create a trained HMM model for testing persistence."""
    np.random.seed(42)

    # Create test features
    n_samples = 50
    n_features = 3

    # Create features with structure
    features = np.random.normal(0, 1, (n_samples, n_features))
    for i in range(n_samples):
        state = i % 3
        if state == 0:
            features[i] += [1.0, 0.5, -0.3]
        elif state == 1:
            features[i] += [-0.5, 1.0, 0.2]
        else:
            features[i] += [0.2, -0.3, 1.0]

    print(f"  ‚úì Created test features: {features.shape}")

    # Train model
    from model_training import train_model
    from utils.config import HMMConfig

    config = HMMConfig(
        n_states=3,
        covariance_type="diag",
        max_iter=50,
        random_state=42,
        num_restarts=2
    )

    model, scaler, score = train_model(features, config)
    print(f"  ‚úì Trained test model: score={score:.4f}")

    return model, scaler, config, features

def test_basic_save_load():
    """Test basic save and load functionality."""
    print("üîç Testing Basic Save and Load Operations")

    try:
        from model_training.model_persistence import (
            get_model_info,
            load_model,
            save_model,
        )

        model, scaler, config, original_features = create_test_model()

        # Create temporary file
        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as temp_file:
            temp_path = temp_file.name

        try:
            # Test save model
            print("  Testing save_model...")
            metadata = save_model(
                model=model,
                scaler=scaler,
                config=config,
                path=temp_path,
                include_metadata=True,
                overwrite=True
            )
            print(f"    ‚úì Model saved: {temp_path}")
            print(f"    ‚úì Metadata: {metadata.n_states} states, {metadata.n_features} features")

            # Test model info before loading
            info = get_model_info(temp_path)
            if 'n_states' in info:
                print(f"    ‚úì Model info accessible: {info['n_states']} states")
            else:
                print("    ‚úó Model info not accessible")
                return False

            # Test load model
            print("  Testing load_model...")
            loaded_model, loaded_scaler, loaded_config, loaded_metadata = load_model(
                path=temp_path,
                validate_integrity=True,
                validate_functionality=True
            )
            print(f"    ‚úì Model loaded: {loaded_model.n_components} states")

            # Validate loaded components
            if loaded_model.n_components == model.n_components:
                print(f"    ‚úì States match: {loaded_model.n_components}")
            else:
                print(f"    ‚úó States mismatch: {loaded_model.n_components} != {model.n_components}")
                return False

            if loaded_scaler.n_features_in_ == scaler.n_features_in_:
                print(f"    ‚úì Scaler features match: {loaded_scaler.n_features_in_}")
            else:
                print(f"    ‚úó Scaler features mismatch: {loaded_scaler.n_features_in_} != {scaler.n_features_in_}")
                return False

            if loaded_config.n_states == config.n_states:
                print(f"    ‚úì Config states match: {loaded_config.n_states}")
            else:
                print(f"    ‚úó Config states mismatch: {loaded_config.n_states} != {config.n_states}")
                return False

            # Test functional equivalence
            print("  Testing functional equivalence...")

            # Get predictions from original and loaded models
            from model_training import predict_states

            # Make sure we pass the same scaler objects (not arrays)
            original_states_tuple = predict_states(model, original_features, scaler)
            original_states = original_states_tuple[0]  # Get just the state sequence

            loaded_states_tuple = predict_states(loaded_model, original_features, loaded_scaler)
            loaded_states = loaded_states_tuple[0]  # Get just the state sequence

            if np.array_equal(original_states, loaded_states):
                print(f"    ‚úì Predictions identical: {len(original_states)} states")
            else:
                print(f"    ‚úó Predictions differ: {np.sum(original_states != loaded_states)} differences")
                return False

            return True

        finally:
            # Clean up temporary file
            if os.path.exists(temp_path):
                os.unlink(temp_path)

    except Exception as e:
        print(f"  ‚úó Basic save/load test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_integrity_validation():
    """Test integrity validation mechanisms."""
    print("\nüîç Testing Integrity Validation")

    try:
        from model_training.model_persistence import (
            generate_model_hash,
            load_model,
            save_model,
        )

        model, scaler, config, features = create_test_model()

        # Generate original hash
        original_hash = generate_model_hash(model, scaler)
        print(f"  ‚úì Original model hash: {original_hash[:16]}...")

        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as temp_file:
            temp_path = temp_file.name

            try:
                # Save model normally
                save_model(model, scaler, config, temp_path, overwrite=True)

                # Load and validate integrity
                loaded_model, loaded_scaler, loaded_config, _ = load_model(
                    temp_path, validate_integrity=True, validate_functionality=False
                )
                print("    ‚úì Integrity validation passed")

                # Test corruption simulation by manually corrupting file
                print("  Testing corruption detection...")

                # Load and corrupt the data
                import pickle
                with open(temp_path, 'rb') as f:
                    data = pickle.load(f)

                # Remove a required key
                corrupted_data = data.copy()
                del corrupted_data['scaler']

                # Save corrupted data
                with open(temp_path, 'wb') as f:
                    pickle.dump(corrupted_data, f)

                # Try to load corrupted file
                try:
                    load_model(temp_path, validate_integrity=True)
                    print("    ‚úó Should have detected corruption")
                    return False
                except ValueError as e:
                    if "missing required keys" in str(e):
                        print(f"    ‚úì Corruption detected: {e}")
                    else:
                        print(f"    ‚ö† Different error: {e}")
                except Exception as e:
                    print(f"    ‚ö† Unexpected error: {e}")

                return True

            finally:
                if os.path.exists(temp_path):
                    os.unlink(temp_path)

    except Exception as e:
        print(f"  ‚úó Integrity validation test failed: {e}")
        return False

def test_error_handling():
    """Test error handling for edge cases."""
    print("\nüîç Testing Error Handling")

    try:
        from model_training.model_persistence import (
            delete_model,
            load_model,
            save_model,
        )

        model, scaler, config, features = create_test_model()

        # Test loading non-existent file
        print("  Testing non-existent file...")
        try:
            load_model("/non/existent/path/model.pkl")
            print("    ‚úó Should have raised FileNotFoundError")
            return False
        except FileNotFoundError:
            print("    ‚úì FileNotFoundError raised correctly")

        # Test loading invalid file
        print("  Testing invalid file format...")
        with tempfile.NamedTemporaryFile(suffix='.txt', delete=False) as temp_file:
            temp_path = temp_file.name

            try:
                # Write invalid data
                with open(temp_path, 'w') as f:
                    f.write("This is not a pickle file")

                try:
                    load_model(temp_path)
                    print("    ‚úó Should have raised ValueError")
                    return False
                except ValueError:
                    print("    ‚úì Invalid file format detected")
            finally:
                if os.path.exists(temp_path):
                    os.unlink(temp_path)

        # Test delete non-existent file
        print("  Testing delete non-existent file...")
        try:
            delete_model("/non/existent/path/model.pkl", confirm=False)
            print("    ‚úó Should have raised FileNotFoundError")
            return False
        except FileNotFoundError:
            print("    ‚úì FileNotFoundError raised correctly")

        # Test copy to existing file without overwrite
        print("  Testing copy without overwrite...")
        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as temp_file1:
            temp_path1 = temp_file.name
            with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as temp_file2:
                temp_path2 = temp_file2.name

                try:
                    # Save a model to first file
                    save_model(model, scaler, config, temp_path1, overwrite=True)

                    # Try to copy to existing file without overwrite
                    from model_training.model_persistence import copy_model
                    copy_model(temp_path1, temp_path2, overwrite=False)
                    print("    ‚úó Should have raised FileExistsError")
                    return False
                except FileExistsError:
                    print("    ‚úì FileExistsError raised correctly")
                finally:
                    for temp_path in [temp_path1, temp_path2]:
                        if os.path.exists(temp_path):
                            os.unlink(temp_path)

        return True

    except Exception as e:
        print(f"  ‚úó Error handling test failed: {e}")
        return False

def test_metadata_functionality():
    """Test metadata functionality."""
    print("\nüîç Testing Metadata Functionality")

    try:
        from model_training.model_persistence import (
            ModelMetadata,
            get_model_info,
            load_model,
            save_model,
        )

        model, scaler, config, features = create_test_model()

        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as temp_file:
            temp_path = temp_file.name

            try:
                # Save with metadata
                print("  Testing save with metadata...")
                metadata = save_model(model, scaler, config, temp_path, include_metadata=True, overwrite=True)

                if isinstance(metadata, ModelMetadata):
                    print(f"    ‚úì Metadata object created: {metadata.model_type}")
                    print(f"    ‚úì Metadata fields: {metadata.n_states} states, {metadata.n_features} features")
                    print(f"    ‚úì Library versions: {list(metadata.library_versions.keys())}")
                else:
                    print("    ‚úó Metadata not properly created")
                    return False

                # Test load without metadata
                loaded_model, loaded_scaler, loaded_config, loaded_metadata = load_model(temp_path)

                if loaded_metadata is not None:
                    print(f"    ‚úì Metadata loaded: {loaded_metadata.n_states} states")
                else:
                    print("    ‚úì No metadata loaded (expected for this test)")

                # Test model info
                info = get_model_info(temp_path)
                if 'metadata' in info and info['metadata'] is not None:
                    print(f"    ‚úì Info contains metadata: {info['metadata']['model_type']}")
                else:
                    print("    ‚ö† No metadata in info (file might not have metadata)")

                # Test saving without metadata
                temp_path_no_meta = temp_path.replace('.pkl', '_no_meta.pkl')
                save_model(model, scaler, config, temp_path_no_meta, include_metadata=False)

                info_no_meta = get_model_info(temp_path_no_meta)
                if info_no_meta.get('has_metadata') is False:
                    print("    ‚úì No metadata flag set correctly")
                else:
                    print("    ‚úó Metadata flag incorrect")
                    return False

                # Clean up
                if os.path.exists(temp_path_no_meta):
                    os.unlink(temp_path_no_meta)

                return True

            finally:
                if os.path.exists(temp_path):
                    os.unlink(temp_path)

    except Exception as e:
        print(f"  ‚úó Metadata functionality test failed: {e}")
        return False

def test_file_management():
    """Test file management utilities."""
    print("\nÔøΩMocking Testing File Management Utilities")

    try:
        from model_training.hmm_trainer import HMMConfig
        from model_training.model_persistence import (
            copy_model,
            delete_model,
            list_saved_models,
            save_model,
        )

        model, scaler, config, features = create_test_model()

        with tempfile.TemporaryDirectory() as temp_dir:
            temp_dir_path = Path(temp_dir)

            # Save multiple models
            print("  Testing multiple model management...")
            saved_files = []
            for i in range(3):
                # Modify config for each model
                test_config = HMMConfig(
                    n_states=2 + i,
                    covariance_type="diag",
                    max_iter=30,
                    random_state=42 + i,
                    num_restarts=1
                )

                file_path = temp_dir_path / f"model_{i}.pkl"
                save_model(model, scaler, test_config, file_path, overwrite=True)
                saved_files.append(file_path)
                print(f"    ‚úì Saved model {i}: {file_path.name}")

            # List models
            print("  Testing list_saved_models...")
            models_list = list_saved_models(temp_dir_path)

            if len(models_list) == 3:
                print(f"    ‚úì Found {len(models_list)} models")
            else:
                print(f"    ‚úó Expected 3 models, found {len(models_list)}")
                return False

            # Verify all models in list
            for model_path_str, model_info in models_list.items():
                if 'metadata' in model_info and model_info['metadata'] is not None:
                    metadata = model_info['metadata']
                    if hasattr(metadata, 'n_states'):
                        print(f"    ‚úì Model {Path(model_path_str).name}: {metadata.n_states} states")
                    else:
                        print(f"    ‚ö† Model {Path(model_path_str).name}: Metadata has no n_states attribute")
                else:
                    print(f"    ‚ö† Model {Path(model_path_str).name}: No metadata")

            # Test copy functionality
            print("  Testing copy functionality...")
            copy_path = temp_dir_path / "copied_model.pkl"

            copy_model(saved_files[0], copy_path, overwrite=False)
            print(f"    ‚úì Copied model: {copy_path.name}")

            # Verify copy exists
            if copy_path.exists():
                print("    ‚úì Copy verified")
            else:
                print("    ‚úó Copy failed")
                return False

            # Test delete functionality
            print("  Testing delete functionality...")
            deleted = delete_model(copy_path, confirm=False)

            if deleted and not copy_path.exists():
                print("    ‚úì Model deleted successfully")
            else:
                print("    ‚úó Model deletion failed")
                return False

            # Verify original models still exist
            remaining_models = list_saved_models(temp_dir_path)
            if len(remaining_models) == 3:  # Original models should still exist
                print(f"    ‚úì Original models preserved: {len(remaining_models)}")
            else:
                print(f"    ‚úó Original models missing: expected 3, found {len(remaining_models)}")
                return False

            return True

    except Exception as e:
        print(f"  ‚úó File management test failed: {e}")
        return False

def test_library_versions():
    """Test library version tracking."""
    print("\nüîç Testing Library Version Tracking")

    try:
        from model_training.model_persistence import get_library_versions

        versions = get_library_versions()
        print(f"  ‚úì Library versions retrieved: {len(versions)} libraries")

        # Check for expected libraries
        expected_libraries = ['numpy', 'sklearn']
        found_libraries = []
        for lib in expected_libraries:
            if lib in versions:
                found_libraries.append(lib)
                print(f"    ‚úì {lib}: {versions[lib]}")
            else:
                print(f"    ‚ö† {lib}: not found")

        if 'hmmlearn' in versions:
            print(f"    ‚úì hmmlearn: {versions['hmmlearn']}")

        if len(found_libraries) >= 2:
            print(f"    ‚úì Essential libraries tracked: {found_libraries}")
            return True
        else:
            print(f"    ‚ö† Missing essential libraries: {expected_libraries}")
            return False

    except Exception as e:
        print(f"  ‚úó Library version test failed: {e}")
        return False

def test_end_to_end_workflow():
    """Test complete end-to-end workflow."""
    print("\nüîç Testing End-to-End Workflow")

    try:
        from model_training import predict_states_comprehensive, train_model
        from model_training.model_persistence import load_model, save_model
        from utils.config import HMMConfig

        # Step 1: Create and train model
        print("  Step 1: Training HMM model...")
        np.random.seed(42)
        features = np.random.normal(0, 1, (30, 3))

        config = HMMConfig(n_states=3, num_restarts=2)
        original_model, original_scaler, original_score = train_model(features, config)
        print(f"    ‚úì Model trained: score={original_score:.4f}")

        # Step 2: Perform inference
        print("  Step 2: Performing inference...")
        original_result = predict_states_comprehensive(original_model, original_scaler, features)
        print(f"    ‚úì Inference completed: {original_result.n_samples} samples")

        # Step 3: Save model
        print("  Step 3: Saving model...")
        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as temp_file:
            temp_path = temp_file.name

            try:
                metadata = save_model(original_model, original_scaler, config, temp_path, overwrite=True)
                print(f"    ‚úì Model saved: {temp_path}")

                # Step 4: Load model
                print("  Step 4: Loading model...")
                loaded_model, loaded_scaler, loaded_config, loaded_metadata = load_model(
                    temp_path,
                    validate_integrity=True,
                    validate_functionality=True
                )
                print(f"    ‚úì Model loaded: {loaded_model.n_components} states")

                # Step 5: Verify inference consistency
                print("  Step 5: Verifying inference consistency...")
                loaded_result = predict_states_comprehensive(loaded_model, loaded_scaler, features)

                if np.array_equal(original_result.states, loaded_result.states):
                    print(f"    ‚úì Inference results identical: {len(original_result.states)} states")
                else:
                    print(f"    ‚úó Inference results differ: {np.sum(original_result.states != loaded_result.states)} differences")
                    return False

                # Step 6: Verify metadata
                print("  Step 6: Verifying metadata...")
                if loaded_metadata is not None:
                    if loaded_metadata.n_states == original_model.n_components:
                        print(f"    ‚úì Metadata consistent: {loaded_metadata.n_states} states")
                    else:
                        print(f"    ‚úó Metadata inconsistent: {loaded_metadata.n_states} != {original_model.n_components}")
                        return False
                else:
                    print("    ‚ö† No metadata available")

                print("  ‚úì End-to-end workflow successful")
                return True

            finally:
                if os.path.exists(temp_path):
                    os.unlink(temp_path)

    except Exception as e:
        print(f"  ‚úó End-to-end workflow test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run all model persistence tests."""
    print("=" * 70)
    print("HMM MODEL PERSISTENCE TESTS (Task 5.2)")
    print("=" * 70)

    tests = [
        test_basic_save_load,
        test_integrity_validation,
        test_error_handling,
        test_metadata_functionality,
        test_file_management,
        test_library_versions,
        test_end_to_end_workflow,
    ]

    passed = 0
    total = len(tests)

    for test in tests:
        if test():
            passed += 1

    print("\n" + "=" * 70)
    print(f"Model Persistence Tests: {passed}/{total} test suites passed")

    if passed == total:
        print("üéâ Task 5.2 FULLY COMPLIANT!")
        print("\n‚úÖ Key Achievements:")
        print("   ‚úì Basic save/load operations with pickle serialization")
        print("   ‚úì Integrity validation with SHA-256 hash checking")
        print("   ‚Ä¢ Required keys validation")
        print("   ‚Ä¢ Model hash verification")
        print("   ‚úì Configuration reconstruction from dictionaries")
        print("   ‚úì Model functionality validation after loading")
        print("   ‚úì Comprehensive metadata tracking")
        print("   ‚Ä¢ Model type, parameters, and scores")
        print("   ‚Ä¢ Library version tracking for reproducibility")
        print("   ‚Ä¢ File size and timestamp recording")
        print("   ‚úì Robust error handling")
        print("   ‚Ä¢ File existence validation")
        print("   ‚Ä¢ Corruption detection and recovery")
        print("   ‚úì File management utilities")
        print("   ‚Ä¢ Multiple model listing")
        print("   ‚Ä¢ Copy and delete operations")
        print("   ‚úì Library version tracking")
        print("   ‚úì End-to-end workflow validation")
        print("   ‚Ä¢ Train ‚Üí Save ‚Üí Load ‚Üí Inference consistency")
        print("   ‚Ä¢ Metadata preservation across cycles")
        print("\nüöÄ Model persistence ready for production!")
        return 0
    else:
        print("‚ùå Some test suites failed")
        return 1

if __name__ == "__main__":
    exit(main())
